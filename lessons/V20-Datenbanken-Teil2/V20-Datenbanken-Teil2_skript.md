# V20 ‚Äì Datenbanken ‚Äì Teil 2

**Vorlesungsinhalte:**
- Informatik-Theorie: Datenbanken ‚Äì Teil 2 (Normalisierung, Indizes, Transaktionen, NoSQL)
- Python-Praxis: Datenbankverbindung & SQL ‚Äì Teil 2 (Prepared Statements, UPDATE/DELETE, Transaktionen, Context Manager)

---

## üìö Informatik-Theorie: Datenbanken ‚Äì Teil 2

### Normalisierung relationaler Datenbanken

Die **Normalisierung** ist ein systematischer Prozess zur Strukturierung relationaler Datenbanken, um Redundanz zu minimieren und Datenintegrit√§t zu maximieren. Ziel ist es, Anomalien bei INSERT, UPDATE und DELETE zu vermeiden. Edgar F. Codd definierte die ersten drei Normalformen (1NF, 2NF, 3NF), die in der Praxis am h√§ufigsten angewendet werden. Weitere Normalformen (BCNF, 4NF, 5NF) existieren, werden aber seltener ben√∂tigt.

> [!NOTE]
> **Normalisierung** bedeutet, Datenstrukturen so zu organisieren, dass jede Information nur einmal gespeichert wird. Dies geschieht durch Aufteilung von Tabellen und Verwendung von Fremdschl√ºsseln. Der Prozess folgt strengen Regeln, die schrittweise angewendet werden.

#### Warum Normalisierung wichtig ist

Unnormalisierte Datenbanken leiden unter drei Hauptproblemen:

**Update-Anomalie**: Wenn dieselbe Information mehrfach gespeichert ist, muss sie an mehreren Stellen ge√§ndert werden. Wird eine Stelle vergessen, entsteht Inkonsistenz.

**Insert-Anomalie**: Manchmal k√∂nnen Daten nicht eingef√ºgt werden, weil abh√§ngige Daten fehlen. Beispiel: Ein Lieferant kann nicht gespeichert werden, bevor ein Produkt existiert, das er liefert.

**Delete-Anomalie**: Das L√∂schen eines Datensatzes kann unbeabsichtigt andere Informationen entfernen. Beispiel: Wird das letzte Produkt eines Lieferanten gel√∂scht, gehen alle Lieferanteninformationen verloren.

> [!TIP]
> **Praktisches Beispiel f√ºr Anomalien**:
> 
> Tabelle `Bestellungen` (unnormalisiert):
> 
> | Bestell_ID | Kunde_Name | Kunde_Adresse   | Artikel_Name | Artikel_Preis | Menge |
> |------------|------------|-----------------|--------------|---------------|-------|
> | 1          | M√ºller AG  | Hauptstr. 10    | Schraube M8  | 0.15          | 1000  |
> | 2          | M√ºller AG  | Hauptstr. 10    | Mutter M8    | 0.10          | 500   |
> | 3          | Schmidt GmbH | Bahnhofstr. 5 | Schraube M8  | 0.15          | 200   |
> 
> **Update-Anomalie**: M√ºller AG zieht um ‚Üí Adresse muss in zwei Zeilen ge√§ndert werden.  
> **Delete-Anomalie**: Wird Bestellung 3 gel√∂scht, verlieren wir Schmidt GmbH komplett.  
> **Insert-Anomalie**: Wir k√∂nnen keinen neuen Kunden anlegen, bevor er etwas bestellt hat.

#### Erste Normalform (1NF)

Eine Tabelle ist in **1NF**, wenn folgende Bedingungen erf√ºllt sind:

1. **Atomare Werte**: Jede Zelle enth√§lt genau einen Wert (keine Listen, Mengen oder verschachtelten Strukturen).
2. **Eindeutige Spaltennamen**: Jede Spalte hat einen eindeutigen Namen.
3. **Gleicher Datentyp pro Spalte**: Alle Werte in einer Spalte haben denselben Typ.
4. **Eindeutige Zeilen**: Es gibt keine Duplikate (Prim√§rschl√ºssel vorhanden).

> [!WARNING]
> **H√§ufiger Fehler**: Mehrere Werte in einer Zelle speichern (z.B. `"Telefon1, Telefon2, Telefon3"`). Dies verletzt 1NF und macht Abfragen extrem umst√§ndlich.

**Beispiel f√ºr Nicht-1NF**:

Tabelle `Mitarbeiter`:
| Mitarbeiter_ID | Name   | Telefonnummern              |
|----------------|--------|-----------------------------|
| 1              | M√ºller | +49 123 456, +49 789 012    |
| 2              | Schmidt| +49 555 999                 |

Die Spalte `Telefonnummern` enth√§lt mehrere Werte ‚Üí **verletzt 1NF**.

**L√∂sung: In 1NF umwandeln**:

Tabelle `Mitarbeiter`:
| Mitarbeiter_ID | Name    |
|----------------|---------|
| 1              | M√ºller  |
| 2              | Schmidt |

Tabelle `Telefonnummern`:
| Telefon_ID | Mitarbeiter_ID | Nummer         | Typ     |
|------------|----------------|----------------|---------|
| 1          | 1              | +49 123 456    | Mobil   |
| 2          | 1              | +49 789 012    | Festnetz|
| 3          | 2              | +49 555 999    | Mobil   |

Jetzt ist jede Zelle atomar, und wir k√∂nnen einfach nach Telefonnummern suchen oder filtern.

#### Zweite Normalform (2NF)

Eine Tabelle ist in **2NF**, wenn sie:

1. **In 1NF** ist.
2. **Keine partiellen Abh√§ngigkeiten** hat: Jedes Nicht-Schl√ºssel-Attribut ist von **allen Teilen** des Prim√§rschl√ºssels abh√§ngig, nicht nur von einem Teil.

Diese Regel ist nur relevant bei **zusammengesetzten Prim√§rschl√ºsseln** (bestehend aus mehreren Spalten).

> [!NOTE]
> Eine **partielle Abh√§ngigkeit** liegt vor, wenn ein Nicht-Schl√ºssel-Attribut nur von einem Teil eines zusammengesetzten Prim√§rschl√ºssels abh√§ngt. Dies f√ºhrt zu Redundanz.

**Beispiel f√ºr Nicht-2NF**:

Tabelle `Bestellpositionen`:
| Bestell_ID | Artikel_ID | Artikel_Name | Artikel_Preis | Menge |
|------------|------------|--------------|---------------|-------|
| 1          | 10         | Schraube M8  | 0.15          | 1000  |
| 1          | 11         | Mutter M8    | 0.10          | 500   |
| 2          | 10         | Schraube M8  | 0.15          | 200   |

Prim√§rschl√ºssel: `(Bestell_ID, Artikel_ID)` (zusammengesetzt)

**Problem**: `Artikel_Name` und `Artikel_Preis` h√§ngen nur von `Artikel_ID` ab, nicht von der gesamten Kombination `(Bestell_ID, Artikel_ID)`. Dies ist eine **partielle Abh√§ngigkeit** ‚Üí verletzt 2NF.

**Folge**: "Schraube M8" und ihr Preis werden mehrfach gespeichert (Zeile 1 und 3). √Ñndert sich der Preis, muss er in allen Zeilen aktualisiert werden.

**L√∂sung: In 2NF umwandeln**:

Tabelle `Bestellpositionen`:
| Bestell_ID | Artikel_ID | Menge |
|------------|------------|-------|
| 1          | 10         | 1000  |
| 1          | 11         | 500   |
| 2          | 10         | 200   |

Tabelle `Artikel`:
| Artikel_ID | Artikel_Name | Artikel_Preis |
|------------|--------------|---------------|
| 10         | Schraube M8  | 0.15          |
| 11         | Mutter M8    | 0.10          |

Jetzt sind alle Nicht-Schl√ºssel-Attribute vom gesamten Prim√§rschl√ºssel abh√§ngig. Artikelinformationen stehen nur noch einmal in der Tabelle `Artikel`.

#### Dritte Normalform (3NF)

Eine Tabelle ist in **3NF**, wenn sie:

1. **In 2NF** ist.
2. **Keine transitiven Abh√§ngigkeiten** hat: Kein Nicht-Schl√ºssel-Attribut ist von einem anderen Nicht-Schl√ºssel-Attribut abh√§ngig.

> [!NOTE]
> Eine **transitive Abh√§ngigkeit** liegt vor, wenn ein Nicht-Schl√ºssel-Attribut B von einem anderen Nicht-Schl√ºssel-Attribut A abh√§ngt, das wiederum vom Prim√§rschl√ºssel abh√§ngt. Schema: Prim√§rschl√ºssel ‚Üí A ‚Üí B. Dies f√ºhrt zu Redundanz.

**Beispiel f√ºr Nicht-3NF**:

Tabelle `Mitarbeiter`:
| Mitarbeiter_ID | Name    | Abteilung_ID | Abteilung_Name | Abteilung_Standort |
|----------------|---------|--------------|----------------|--------------------|
| 1              | M√ºller  | 10           | Produktion     | Halle A            |
| 2              | Schmidt | 10           | Produktion     | Halle A            |
| 3              | Weber   | 20           | Qualit√§tssicherung | Geb√§ude B      |

Prim√§rschl√ºssel: `Mitarbeiter_ID`

**Problem**: `Abteilung_Name` und `Abteilung_Standort` h√§ngen von `Abteilung_ID` ab, nicht direkt von `Mitarbeiter_ID`. Transitive Abh√§ngigkeit: `Mitarbeiter_ID ‚Üí Abteilung_ID ‚Üí Abteilung_Name, Abteilung_Standort` ‚Üí verletzt 3NF.

**Folge**: "Produktion" und "Halle A" werden mehrfach gespeichert (Zeile 1 und 2). √Ñndert sich der Standort der Produktion, muss dies in allen Zeilen ge√§ndert werden.

**L√∂sung: In 3NF umwandeln**:

Tabelle `Mitarbeiter`:
| Mitarbeiter_ID | Name    | Abteilung_ID |
|----------------|---------|--------------|
| 1              | M√ºller  | 10           |
| 2              | Schmidt | 10           |
| 3              | Weber   | 20           |

Tabelle `Abteilungen`:
| Abteilung_ID | Abteilung_Name       | Abteilung_Standort |
|--------------|----------------------|--------------------|
| 10           | Produktion           | Halle A            |
| 20           | Qualit√§tssicherung   | Geb√§ude B          |

Jetzt ist jede Information nur einmal gespeichert, und Updates sind konsistent.

> [!TIP]
> **Faustregel f√ºr Normalisierung**:
> - **1NF**: Eine Spalte = ein Wert (keine Listen!)
> - **2NF**: Keine halben Schl√ºssel (bei zusammengesetzten Prim√§rschl√ºsseln)
> - **3NF**: Nicht-Schl√ºssel-Attribute h√§ngen nur vom Prim√§rschl√ºssel ab, nicht voneinander
> 
> In der Praxis sind die meisten gut entworfenen Datenbanken in 3NF.

#### Denormalisierung: Wann macht Abweichung Sinn?

Obwohl Normalisierung Redundanz reduziert, kann sie Performance-Probleme verursachen. Stark normalisierte Datenbanken erfordern viele **JOINs**, um Daten zusammenzuf√ºgen. Bei sehr gro√üen Datenmengen oder komplexen Queries kann dies langsam werden.

**Denormalisierung** bedeutet bewusst Redundanz einzuf√ºhren, um Abfragen zu beschleunigen. Dies sollte nur nach sorgf√§ltiger Analyse geschehen.

**Typische Szenarien f√ºr Denormalisierung**:

**Read-Heavy Workloads**: Wenn Daten sehr viel h√§ufiger gelesen als geschrieben werden, k√∂nnen berechnete Werte oder duplizierte Informationen die Performance verbessern.

**Reporting/Analytics**: Data Warehouses verwenden oft **Star-Schema** oder **Snowflake-Schema**, die teilweise denormalisiert sind, um komplexe Aggregationen zu beschleunigen.

**Caching**: Berechnete Werte (z.B. Summen, Durchschnitte) k√∂nnen vorberechnet und gespeichert werden, statt sie bei jeder Abfrage neu zu berechnen.

> [!WARNING]
> **Denormalisierung ist ein Trade-Off**: Man gewinnt Leseperformance, verliert aber Konsistenzgarantien und erh√∂ht Komplexit√§t bei Updates. Verwende Denormalisierung nur, wenn Performance-Messungen zeigen, dass normalisierte Abfragen zu langsam sind.

**Beispiel**: In einer E-Commerce-Datenbank k√∂nnte man `Anzahl_Bestellungen` und `Umsatz_Gesamt` direkt in der `Kunden`-Tabelle speichern, statt sie jedes Mal aus der `Bestellungen`-Tabelle zu berechnen. Dies erfordert aber Trigger oder Anwendungslogik, um diese Felder bei jeder Bestellung zu aktualisieren.

### Indizes zur Performance-Optimierung

Ein **Index** ist eine Datenstruktur, die schnelle Zugriffe auf Zeilen einer Tabelle basierend auf bestimmten Spalten erm√∂glicht. Ohne Indizes muss das DBMS bei Abfragen die gesamte Tabelle durchsuchen (**Full Table Scan**), was bei gro√üen Tabellen sehr langsam ist.

> [!NOTE]
> Ein **Index** funktioniert √§hnlich wie ein Stichwortverzeichnis in einem Buch: Statt das gesamte Buch zu durchsuchen, schl√§gt man im Index nach und findet direkt die relevanten Seiten. Technisch sind Indizes meist **B-Tree** oder **Hash-Tabellen**.

#### Wie funktionieren Indizes?

Wenn ein Index auf Spalte `X` existiert, erstellt das DBMS eine sortierte Liste aller Werte von `X` zusammen mit Zeigern auf die entsprechenden Zeilen. Bei einer Abfrage `WHERE X = 'Wert'` kann das DBMS bin√§re Suche verwenden, statt alle Zeilen zu durchlaufen.

**Beispiel ohne Index**:

```sql
SELECT * FROM Maschinen WHERE Maschinentyp = 'CNC-Fr√§se';
```

Ohne Index: DBMS muss alle Zeilen der Tabelle `Maschinen` durchgehen ‚Üí O(n) Zeitkomplexit√§t.

**Beispiel mit Index**:

```sql
CREATE INDEX idx_maschinentyp ON Maschinen(Maschinentyp);
SELECT * FROM Maschinen WHERE Maschinentyp = 'CNC-Fr√§se';
```

Mit Index: DBMS nutzt den Index ‚Üí O(log n) Zeitkomplexit√§t f√ºr Suche, dann direkter Zugriff auf Zeilen.

> [!TIP]
> **B-Tree-Indizes** (der Standard in den meisten DBMS):
> - Sortierte Baumstruktur mit garantiert ausbalancierter H√∂he
> - Suche: O(log n)
> - Einf√ºgen/L√∂schen: O(log n)
> - Gut f√ºr Bereichsabfragen (`WHERE X BETWEEN a AND b`)
> - Gut f√ºr Sortierungen (`ORDER BY X`)

#### Wann sollte man Indizes erstellen?

Indizes beschleunigen **SELECT**, **WHERE**, **JOIN** und **ORDER BY** Operationen, verlangsamen aber **INSERT**, **UPDATE** und **DELETE**, weil der Index aktualisiert werden muss.

**Gute Kandidaten f√ºr Indizes**:

**Prim√§rschl√ºssel**: Wird automatisch indexiert (in den meisten DBMS).

**Fremdschl√ºssel**: Beschleunigt JOINs erheblich. Manche DBMS indexieren Fremdschl√ºssel automatisch, andere nicht.

**Spalten in WHERE-Klauseln**: Wenn eine Spalte h√§ufig gefiltert wird (`WHERE Status = 'aktiv'`), hilft ein Index.

**Spalten in JOIN-Bedingungen**: Wenn Tabellen oft √ºber eine Spalte verkn√ºpft werden, sollte diese indexiert sein.

**Spalten in ORDER BY/GROUP BY**: Indizes k√∂nnen Sortierungen beschleunigen oder √ºberfl√ºssig machen.

**Schlechte Kandidaten f√ºr Indizes**:

**Spalten mit wenigen unterschiedlichen Werten** (Low Cardinality): Ein Index auf eine Spalte mit nur zwei Werten (z.B. `Geschlecht: M/F`) bringt kaum Vorteil.

**Selten genutzte Spalten**: Wenn eine Spalte nie in WHERE, JOIN oder ORDER BY vorkommt, ist ein Index verschwendeter Speicher.

**Sehr kleine Tabellen**: Bei Tabellen mit nur wenigen Dutzend Zeilen ist ein Full Table Scan schneller als Index-Zugriff (wegen Overhead).

> [!WARNING]
> **Zu viele Indizes schaden**: Jeder Index ben√∂tigt Speicherplatz und verlangsamt Schreiboperationen. Eine Tabelle mit 10 Indizes kann bei INSERT/UPDATE extrem langsam werden. Erstelle Indizes nur basierend auf tats√§chlichen Query-Patterns.

#### Index-Typen

**Einspalten-Index (Single-Column Index)**:

```sql
CREATE INDEX idx_baujahr ON Maschinen(Baujahr);
```

Beschleunigt Abfragen wie `WHERE Baujahr > 2015`.

**Mehrspaltiger Index (Composite Index)**:

```sql
CREATE INDEX idx_typ_baujahr ON Maschinen(Maschinentyp, Baujahr);
```

Beschleunigt Abfragen wie `WHERE Maschinentyp = 'Fr√§se' AND Baujahr > 2015`. Die Reihenfolge der Spalten ist wichtig: Dieser Index hilft auch bei `WHERE Maschinentyp = 'Fr√§se'`, aber **nicht** bei `WHERE Baujahr > 2015` alleine.

**Unique Index**:

```sql
CREATE UNIQUE INDEX idx_seriennummer ON Maschinen(Seriennummer);
```

Erzwingt Eindeutigkeit (wie ein UNIQUE Constraint) und bietet zus√§tzliche Performance.

**Partial Index (PostgreSQL)**:

```sql
CREATE INDEX idx_aktive_maschinen ON Maschinen(Maschinen_ID) WHERE Aktiv = true;
```

Indexiert nur Zeilen, die eine Bedingung erf√ºllen. Spart Speicher und beschleunigt Abfragen auf dieser Teilmenge.

**Volltext-Index (Full-Text Index)**:

```sql
CREATE FULLTEXT INDEX idx_beschreibung ON Artikel(Beschreibung);
```

F√ºr Textsuche in langen Texten (z.B. Produktbeschreibungen). Unterst√ºtzt Wortsuche, Stemming, Relevanz-Ranking.

> [!TIP]
> **Index-Analyse**: Die meisten DBMS bieten `EXPLAIN` oder `EXPLAIN ANALYZE`, um zu sehen, ob ein Index genutzt wird:
> 
> ```sql
> EXPLAIN SELECT * FROM Maschinen WHERE Maschinentyp = 'CNC-Fr√§se';
> ```
> 
> Output zeigt, ob "Index Scan" oder "Sequential Scan" verwendet wird.

### Transaktionen und ACID-Prinzipien

In produktiven Datenbankanwendungen finden oft mehrere Operationen statt, die logisch zusammengeh√∂ren. Beispiel: Eine Bank√ºberweisung besteht aus zwei Operationen: Geld vom Konto A abbuchen und Geld auf Konto B gutschreiben. Beide Operationen m√ºssen entweder komplett ausgef√ºhrt werden oder gar nicht ‚Äì ein Zwischenzustand (Geld abgebucht, aber nicht gutgeschrieben) w√§re katastrophal.

Eine **Transaktion** ist eine Folge von Datenbankoperationen, die als atomare Einheit behandelt wird. Das DBMS garantiert, dass entweder alle Operationen erfolgreich ausgef√ºhrt werden (**COMMIT**) oder keine (**ROLLBACK**).

> [!NOTE]
> Eine **Transaktion** ist eine Sequenz von SQL-Statements, die als unteilbare Einheit ausgef√ºhrt wird. Transaktionen beginnen mit `BEGIN` (oder implizit beim ersten Statement) und enden mit `COMMIT` (erfolgreich) oder `ROLLBACK` (abgebrochen).

#### Die ACID-Eigenschaften

ACID ist ein Akronym f√ºr vier fundamentale Eigenschaften, die ein transaktionales DBMS garantieren muss:

**Atomicity (Atomarit√§t)**: Eine Transaktion ist unteilbar. Entweder werden alle Operationen ausgef√ºhrt, oder keine. Bei einem Fehler (z.B. Constraint-Verletzung, Systemabsturz) werden alle bisher durchgef√ºhrten Operationen der Transaktion r√ºckg√§ngig gemacht.

**Beispiel**: √úberweisung von 100 ‚Ç¨ von Konto A nach Konto B:

```sql
BEGIN TRANSACTION;
UPDATE Konten SET Saldo = Saldo - 100 WHERE Konto_ID = 1;  -- Konto A
UPDATE Konten SET Saldo = Saldo + 100 WHERE Konto_ID = 2;  -- Konto B
COMMIT;
```

Wenn das zweite UPDATE fehlschl√§gt (z.B. Konto B existiert nicht), wird das erste UPDATE automatisch r√ºckg√§ngig gemacht. Das Geld verschwindet nicht.

**Consistency (Konsistenz)**: Eine Transaktion √ºberf√ºhrt die Datenbank von einem konsistenten Zustand in einen anderen konsistenten Zustand. Alle Integrit√§tsbedingungen (Constraints, Foreign Keys, Check Constraints) m√ºssen am Ende der Transaktion erf√ºllt sein.

**Beispiel**: Ein Fremdschl√ºssel-Constraint verbietet Referenzen auf nicht-existierende Zeilen. Wenn eine Transaktion versucht, ein Wartungsprotokoll f√ºr eine nicht-existierende Maschine zu erstellen, schl√§gt sie fehl, und die Datenbank bleibt konsistent.

**Isolation (Isolierung)**: Parallele Transaktionen beeinflussen sich nicht gegenseitig. Jede Transaktion sieht die Datenbank so, als w√§re sie die einzige Transaktion. Dies verhindert Race Conditions und Inkonsistenzen bei gleichzeitigen Zugriffen.

**Beispiel**: Zwei Benutzer kaufen gleichzeitig das letzte verf√ºgbare Produkt. Ohne Isolation k√∂nnten beide die Menge als "verf√ºgbar" sehen und bestellen. Mit Isolation stellt das DBMS sicher, dass nur eine Transaktion erfolgreich ist.

**Isolation Levels** (von schwach zu stark):
- **READ UNCOMMITTED**: Erlaubt "Dirty Reads" (Lesen von nicht-committeten Daten) ‚Äì kaum verwendet
- **READ COMMITTED**: Liest nur committete Daten ‚Äì Standard in vielen DBMS
- **REPEATABLE READ**: Garantiert, dass wiederholte Reads denselben Wert liefern
- **SERIALIZABLE**: St√§rkste Isolation, als ob Transaktionen nacheinander ablaufen ‚Äì langsamste Option

**Durability (Dauerhaftigkeit)**: Sobald eine Transaktion erfolgreich committed wurde, sind ihre √Ñnderungen dauerhaft gespeichert. Selbst bei Systemabsturz, Stromausfall oder Hardware-Fehler bleiben die Daten erhalten.

**Implementierung**: DBMS verwenden **Write-Ahead Logging (WAL)**: √Ñnderungen werden zun√§chst in ein Log geschrieben (auf Festplatte), bevor sie in die eigentlichen Datenbankdateien √ºbernommen werden. Bei einem Crash kann das DBMS aus dem Log wiederherstellen.

> [!WARNING]
> **Transaktions-Overhead**: Transaktionen haben Performance-Kosten (Logging, Locking). Bei sehr vielen kleinen Transaktionen kann dies zum Flaschenhals werden. Gruppiere zusammenh√§ngende Operationen in eine Transaktion, aber halte Transaktionen so kurz wie m√∂glich.

#### Transaktionen in der Praxis

**Explizite Transaktionen**:

```sql
BEGIN TRANSACTION;  -- Startet Transaktion

UPDATE Lagerbestand SET Menge = Menge - 10 WHERE Artikel_ID = 5;
INSERT INTO Buchungen (Artikel_ID, Menge, Datum) VALUES (5, -10, '2024-12-15');

COMMIT;  -- Speichert √Ñnderungen dauerhaft
```

Falls ein Fehler auftritt:

```sql
ROLLBACK;  -- Macht alle √Ñnderungen seit BEGIN r√ºckg√§ngig
```

**Implizite Transaktionen**: Manche DBMS (z.B. SQLite im Default-Modus) wrappen jedes einzelne Statement automatisch in eine Transaktion. Dies ist sicher, aber ineffizient bei mehreren zusammenh√§ngenden Operationen.

**Autocommit-Modus**: In vielen SQL-Clients ist Autocommit standardm√§√üig aktiviert ‚Äì jedes Statement wird sofort committed. F√ºr Anwendungen sollte Autocommit deaktiviert werden, um Transaktionen explizit zu kontrollieren.

> [!TIP]
> **Best Practice f√ºr Transaktionen**:
> - Beginne Transaktionen so sp√§t wie m√∂glich, committed sie so fr√ºh wie m√∂glich (reduziert Locking-Konflikte)
> - Halte Transaktionen kurz (keine langen Berechnungen zwischen BEGIN und COMMIT)
> - Verwende passenden Isolation Level (nicht immer SERIALIZABLE ‚Äì das ist oft zu restriktiv)
> - In Anwendungen: Verwende Connection Pooling und transaktionale ORM-Features

#### Locks und Concurrency Control

Um Isolation zu garantieren, verwenden DBMS **Locks** (Sperren). Wenn eine Transaktion eine Zeile liest oder √§ndert, kann sie sie f√ºr andere Transaktionen sperren.

**Lock-Typen**:

**Shared Lock (S-Lock, Read Lock)**: Mehrere Transaktionen k√∂nnen gleichzeitig eine Zeile lesen, aber nicht √§ndern. Wird bei `SELECT` gesetzt (abh√§ngig von Isolation Level).

**Exclusive Lock (X-Lock, Write Lock)**: Nur eine Transaktion kann die Zeile √§ndern. Wird bei `UPDATE`, `DELETE`, `INSERT` gesetzt. Blockiert andere Lese- und Schreiboperationen.

**Deadlocks**: Zwei Transaktionen warten gegenseitig aufeinander. Beispiel: Transaktion A sperrt Zeile 1 und wartet auf Zeile 2, Transaktion B sperrt Zeile 2 und wartet auf Zeile 1. Das DBMS erkennt Deadlocks und bricht eine Transaktion ab (mit Fehlermeldung).

> [!WARNING]
> **Deadlock-Vermeidung**:
> - Greife immer in derselben Reihenfolge auf Ressourcen zu (z.B. sortiert nach Prim√§rschl√ºssel)
> - Halte Transaktionen kurz
> - Verwende optimistische Locking statt pessimistische Locks (wenn m√∂glich)

**Optimistisches vs. Pessimistisches Locking**:

**Pessimistisches Locking**: Transaktion sperrt Zeile sofort beim Lesen (`SELECT ... FOR UPDATE`). Andere Transaktionen m√ºssen warten. Sicher, aber langsam bei hohem Parallelit√§tsgrad.

**Optimistisches Locking**: Transaktion liest Zeile ohne Lock. Beim Update pr√ºft sie, ob die Zeile sich zwischenzeitlich ge√§ndert hat (z.B. via Version-Spalte). Falls ja, schl√§gt Update fehl, und Anwendung muss Konflikt aufl√∂sen. Schneller bei geringen Konflikten.

### NoSQL-Datenbanken: Wann und warum?

**NoSQL** (Not Only SQL) ist eine Sammelbezeichnung f√ºr Datenbanksysteme, die das relationale Modell aufgeben zugunsten anderer Datenstrukturen. NoSQL-Datenbanken entstanden ab 2000, um Skalierbarkeits- und Flexibilit√§tsprobleme relationaler Datenbanken bei sehr gro√üen Datenmengen (Big Data) zu l√∂sen.

> [!NOTE]
> **NoSQL** bedeutet **nicht** "kein SQL" (viele NoSQL-Datenbanken bieten SQL-√§hnliche Abfragesprachen), sondern "Not Only SQL" ‚Äì also Datenbanken, die alternative Modelle verwenden.

#### Warum NoSQL?

Relationale Datenbanken haben Grenzen:

**Horizontale Skalierung**: Relationale DBMS sind f√ºr vertikale Skalierung optimiert (gr√∂√üerer Server). Horizontale Skalierung (viele Server) ist schwierig wegen ACID-Garantien und JOINs √ºber mehrere Server.

**Schema-Flexibilit√§t**: Relationale Datenbanken erfordern ein festes Schema. √Ñnderungen am Schema (z.B. neue Spalte) k√∂nnen bei gro√üen Tabellen Stunden dauern und Downtime verursachen.

**Komplexe Datenstrukturen**: Hierarchische oder graphartige Daten (z.B. soziale Netzwerke, verschachtelte JSON-Dokumente) sind in relationalen Tabellen umst√§ndlich zu modellieren.

**Performance bei spezifischen Workloads**: Bestimmte Zugriffspatterns (z.B. Key-Value-Lookups, Graph-Traversierung) sind in spezialisierten NoSQL-Datenbanken schneller.

> [!WARNING]
> **NoSQL ist kein Allheilmittel**: Viele Probleme, die angeblich NoSQL erfordern, sind in Wirklichkeit schlechtes Design oder fehlende Indizes in relationalen Datenbanken. Verwende NoSQL nur, wenn du **nachweislich** an Grenzen relationaler Systeme st√∂√üt.

#### NoSQL-Kategorien

**Key-Value-Stores** (Beispiel: Redis, DynamoDB):
- Einfachstes Modell: Speichert Werte unter Schl√ºsseln (wie ein riesiges Dictionary)
- Sehr schnell f√ºr Lookups (O(1))
- Keine Queries, nur direkter Zugriff per Key
- Verwendung: Caching, Session-Storage, Counters, Echtzeit-Leaderboards

**Document Stores** (Beispiel: MongoDB, CouchDB):
- Speichert semi-strukturierte Dokumente (JSON, BSON, XML)
- Flexibles Schema: Jedes Dokument kann unterschiedliche Felder haben
- Unterst√ºtzt verschachtelte Strukturen
- Query-F√§higkeiten (√§hnlich SQL, aber f√ºr JSON)
- Verwendung: Content Management, Kataloge, User Profiles, Event Logs

**Column-Family Stores** (Beispiel: Apache Cassandra, HBase):
- Speichert Daten spaltenweise statt zeilenweise
- Optimiert f√ºr Schreib-intensive Workloads und gro√üe Datenmengen
- Horizontal skalierbar (Petabyte-Scale)
- Verwendung: Zeitreihen-Daten, IoT-Sensor-Daten, Logs

**Graph Databases** (Beispiel: Neo4j, Amazon Neptune):
- Speichert Knoten (Entities) und Kanten (Beziehungen)
- Optimiert f√ºr Traversierung von Beziehungen
- Query-Sprachen f√ºr Graph-Patterns (z.B. Cypher)
- Verwendung: Soziale Netzwerke, Empfehlungssysteme, Fraud Detection, Wissensgraphen

> [!TIP]
> **NoSQL-Auswahl**:
> - **Redis**: Wenn du ein in-Memory-Cache oder Pub/Sub brauchst
> - **MongoDB**: Wenn du flexible Schemas und JSON-artige Dokumente hast
> - **Cassandra**: Wenn du extrem hohe Schreibraten und lineare Skalierung brauchst
> - **Neo4j**: Wenn dein Datenmodell prim√§r aus Beziehungen besteht (z.B. Freundschaften, Abh√§ngigkeiten)

#### MongoDB Beispiel

MongoDB ist eine der popul√§rsten Document Stores. Daten werden als JSON-√§hnliche Dokumente (BSON) gespeichert.

**Beispiel-Dokument** (Maschinen-Datenbank):

```json
{
  "_id": ObjectId("507f1f77bcf86cd799439011"),
  "Maschinentyp": "CNC-Fr√§se DMU 50",
  "Baujahr": 2018,
  "Standort": "Halle A1",
  "Wartungsintervall_Tage": 90,
  "Letzte_Wartung": ISODate("2024-01-15"),
  "Sensoren": [
    {"Typ": "Temperatur", "Einheit": "Celsius", "Grenzwert": 80},
    {"Typ": "Vibration", "Einheit": "mm/s", "Grenzwert": 5.0}
  ],
  "Status": "aktiv"
}
```

Vorteile gegen√ºber relationaler DB:
- Verschachtelte Arrays (`Sensoren`) direkt speicherbar (keine separate Tabelle)
- Schema kann pro Dokument variieren (manche Maschinen haben andere Sensoren)
- Kein JOIN n√∂tig, um alle Maschineninformationen zu laden

**Query in MongoDB**:

```javascript
db.maschinen.find({ "Baujahr": { "$gt": 2015 }, "Status": "aktiv" })
```

Findet alle aktiven Maschinen mit Baujahr > 2015.

#### CAP-Theorem

Das **CAP-Theorem** (Eric Brewer, 2000) besagt, dass ein verteiltes Datenbanksystem **maximal zwei** der folgenden drei Eigenschaften gleichzeitig garantieren kann:

**Consistency (Konsistenz)**: Alle Knoten sehen dieselben Daten zur selben Zeit. Nach einem Write sehen alle Reads den neuen Wert.

**Availability (Verf√ºgbarkeit)**: Jede Anfrage erh√§lt eine Antwort (erfolgreich oder Fehler), auch wenn Knoten ausfallen.

**Partition Tolerance (Partitionstoleranz)**: System funktioniert auch bei Netzwerkausf√§llen zwischen Knoten (Kommunikation unterbrochen).

Da Netzwerkpartitionen in verteilten Systemen unvermeidbar sind, muss zwischen **CP** (Consistency + Partition Tolerance) und **AP** (Availability + Partition Tolerance) gew√§hlt werden.

**CP-Systeme** (z.B. HBase, MongoDB im Strong Consistency Mode): Bei Netzwerkpartition werden Writes abgelehnt, um Konsistenz zu garantieren. Manche Knoten sind dann nicht verf√ºgbar.

**AP-Systeme** (z.B. Cassandra, DynamoDB): Bei Netzwerkpartition bleiben alle Knoten verf√ºgbar, aber Daten k√∂nnen tempor√§r inkonsistent sein (Eventual Consistency).

> [!WARNING]
> **Eventual Consistency**: In AP-Systemen k√∂nnen nach einem Write kurzzeitig unterschiedliche Werte auf verschiedenen Knoten existieren. Nach einiger Zeit (Millisekunden bis Sekunden) konvergieren sie. F√ºr kritische Anwendungen (Banking, Bestandsverwaltung) ist dies oft inakzeptabel.

### KI und Datenbanken: Vector Databases f√ºr Embeddings

Mit dem Aufstieg von Machine Learning und Large Language Models (LLMs) entstanden spezielle Datenbanken f√ºr **Vektor-Embeddings**. Ein **Embedding** ist eine numerische Repr√§sentation von Daten (Text, Bilder, Audio) als Vektor in hochdimensionalem Raum (z.B. 1536 Dimensionen bei OpenAI's `text-embedding-3-small`).

> [!NOTE]
> Ein **Vector Embedding** ist ein Vektor von Flie√ükommazahlen, der semantische Bedeutung kodiert. √Ñhnliche Konzepte haben √§hnliche Vektoren (gemessen via Cosinus-√Ñhnlichkeit oder Euklidischer Distanz).

#### Warum Vector Databases?

Traditionelle Datenbanken sind ineffizient f√ºr **Similarity Search** (√Ñhnlichkeitssuche) in hochdimensionalen R√§umen. Eine Abfrage wie "Finde die 10 √§hnlichsten Dokumente zu diesem Text" w√ºrde in einer relationalen DB Millionen Vektoren durchrechnen m√ºssen.

**Vector Databases** (z.B. Pinecone, Weaviate, Milvus, Chroma) sind f√ºr diese Workload optimiert:

**Approximate Nearest Neighbor (ANN)**: Statt exakte n√§chste Nachbarn zu finden (langsam), verwenden sie Algorithmen wie HNSW (Hierarchical Navigable Small World) oder IVF (Inverted File Index), um schnell approximative Ergebnisse zu liefern.

**Skalierung**: K√∂nnen Milliarden von Vektoren handhaben und horizontal skalieren.

**Integration mit ML-Pipelines**: Bieten APIs f√ºr Embedding-Generierung und speichern Metadaten zusammen mit Vektoren.

#### Anwendungsf√§lle

**Semantische Suche**: Suche nach "Wartungsanleitung f√ºr Hydrauliksysteme" findet Dokumente, die das Wort "Hydraulik" nicht enthalten, aber semantisch verwandt sind (z.B. "Drucksysteme in Pressen").

**Retrieval-Augmented Generation (RAG)**: LLMs werden mit kontextrelevanten Dokumenten aus Vector Database gef√ºttert, um halluzinationsarme Antworten zu generieren.

**Empfehlungssysteme**: Produkt-Embeddings basierend auf Beschreibungen; finde √§hnliche Produkte.

**Duplikat-Erkennung**: Finde √§hnliche Datens√§tze (z.B. doppelte Kundeneintr√§ge, √§hnliche Support-Tickets).

**Bilderkennung**: Speichere Bild-Embeddings und finde visuell √§hnliche Bilder.

> [!TIP]
> **Vector Database Workflow**:
> 1. Generiere Embeddings mit einem Modell (z.B. OpenAI API, Sentence Transformers)
> 2. Speichere Embeddings + Metadaten in Vector DB
> 3. Bei Query: Generiere Query-Embedding, suche n√§chste Nachbarn
> 4. Gebe Metadaten der gefundenen Vektoren zur√ºck (z.B. Dokument-IDs, Texte)

**Beispiel: Dokumentensuche mit Embeddings**

```python
import openai
import pinecone  # Vector Database

# 1. Dokumente embedden
dokumente = [
    "Wartungsanleitung f√ºr CNC-Fr√§sen",
    "Sicherheitsvorschriften f√ºr Drehmaschinen",
    "Hydrauliksystem-Handbuch"
]

embeddings = [openai.Embedding.create(input=doc, model="text-embedding-3-small")['data'][0]['embedding'] 
              for doc in dokumente]

# 2. In Vector DB speichern
pinecone.init(api_key="...")
index = pinecone.Index("maschinenbau-docs")
index.upsert(vectors=[(f"doc_{i}", emb, {"text": doc}) 
                       for i, (emb, doc) in enumerate(zip(embeddings, dokumente))])

# 3. Suche
query = "Wie warte ich eine Fr√§smaschine?"
query_emb = openai.Embedding.create(input=query, model="text-embedding-3-small")['data'][0]['embedding']
results = index.query(vector=query_emb, top_k=2, include_metadata=True)

# 4. Ergebnisse
for match in results['matches']:
    print(f"Score: {match['score']:.3f} | Text: {match['metadata']['text']}")
```

Output:
```
Score: 0.912 | Text: Wartungsanleitung f√ºr CNC-Fr√§sen
Score: 0.734 | Text: Sicherheitsvorschriften f√ºr Drehmaschinen
```

Die semantische √Ñhnlichkeit zwischen "Fr√§smaschine warten" und "Wartungsanleitung f√ºr CNC-Fr√§sen" wird erkannt, obwohl die W√∂rter unterschiedlich sind.

#### Hybrid-Ans√§tze

Viele produktive Systeme kombinieren relationale Datenbanken mit Vector Databases:

**PostgreSQL + pgvector**: PostgreSQL-Extension f√ºr Vektor-√Ñhnlichkeitssuche. Gut f√ºr kleine bis mittlere Datenmengen (<1 Million Vektoren).

**Elasticsearch + Dense Vectors**: Elasticsearch bietet seit Version 7.x Vector-Suche. Kombiniert Volltext-Suche mit semantischer Suche.

**Weaviate**: Vector Database mit eingebautem Objekt-Schema (wie relationale Tabellen). Unterst√ºtzt Hybrid Search (Keyword + Vektor).

> [!WARNING]
> **Vector Databases sind teuer**: Speicherung und Indexierung hochdimensionaler Vektoren ben√∂tigt viel RAM. Eine Million 1536-dimensionale Vektoren (Float32) = ~6 GB reiner Daten + Index-Overhead. Cloud-Kosten k√∂nnen schnell steigen.

---

## üêç Python-Praxis: Datenbankverbindung & SQL ‚Äì Teil 2

In Vorlesung 19 wurden die Grundlagen von SQLite und SQL eingef√ºhrt: Verbindung zu Datenbanken herstellen, Tabellen erstellen, Daten einf√ºgen und einfache Abfragen durchf√ºhren. In dieser Vorlesung vertiefen wir diese Kenntnisse und behandeln Sicherheitsaspekte, Transaktionen, fortgeschrittene Query-Techniken und Best Practices f√ºr professionelle Datenbankprogrammierung.

### Prepared Statements gegen SQL-Injection

**SQL-Injection** ist eine der gef√§hrlichsten Sicherheitsl√ºcken in Webanwendungen. Sie entsteht, wenn Benutzereingaben unsicher in SQL-Queries eingef√ºgt werden. Ein Angreifer kann dadurch beliebige SQL-Befehle ausf√ºhren, Daten lesen, √§ndern oder l√∂schen.

> [!WARNING]
> **SQL-Injection Beispiel** (NIEMALS SO PROGRAMMIEREN!):
> 
> ```python
> # UNSICHER ‚Äì SQL-Injection m√∂glich!
> maschinen_id = input("Maschinen-ID: ")
> cursor.execute(f"SELECT * FROM Maschinen WHERE Maschinen_ID = {maschinen_id}")
> ```
> 
> Eingabe: `1 OR 1=1` f√ºhrt zu:
> ```sql
> SELECT * FROM Maschinen WHERE Maschinen_ID = 1 OR 1=1
> ```
> Dies gibt **alle** Maschinen zur√ºck, nicht nur ID 1!
> 
> Schlimmerer Fall ‚Äì Eingabe: `1; DROP TABLE Maschinen; --`
> ```sql
> SELECT * FROM Maschinen WHERE Maschinen_ID = 1; DROP TABLE Maschinen; --
> ```
> Dies l√∂scht die gesamte Tabelle!

**Prepared Statements** (auch Parameterized Queries) sind die L√∂sung. Der SQL-Code wird vom DBMS vorkompiliert, Parameter werden separat √ºbergeben und k√∂nnen nicht als Code interpretiert werden.

In Python mit `sqlite3` verwenden wir **`?` Platzhalter**:

```python
import sqlite3

conn = sqlite3.connect('produktionsdb.db')
cursor = conn.cursor()

# SICHER ‚Äì Prepared Statement
maschinen_id = input("Maschinen-ID: ")
cursor.execute('SELECT * FROM Maschinen WHERE Maschinen_ID = ?', (maschinen_id,))
zeilen = cursor.fetchall()
```

Selbst wenn der Benutzer `1 OR 1=1` eingibt, wird dies als String `"1 OR 1=1"` interpretiert, nicht als SQL-Code. Die Abfrage findet keine Maschine mit dieser ID.

> [!NOTE]
> **Wichtig**: Der zweite Parameter von `.execute()` muss ein **Tupel** sein, auch bei nur einem Parameter: `(wert,)` mit Komma! `(wert)` ohne Komma ist kein Tupel, sondern nur eine geklammerte Expression.

**Beispiel mit mehreren Parametern**:

```python
# Maschine einf√ºgen
cursor.execute('''
    INSERT INTO Maschinen (Name, Typ, Baujahr, Standort) 
    VALUES (?, ?, ?, ?)
''', ('CNC-Fr√§se DMU 50', 'Fr√§se', 2020, 'Halle A1'))
conn.commit()
```

**Named Placeholders** (Alternative Syntax mit Dictionaries):

```python
cursor.execute('''
    SELECT * FROM Maschinen 
    WHERE Typ = :typ AND Baujahr > :jahr
''', {'typ': 'Fr√§se', 'jahr': 2015})
```

Named Placeholders sind lesbarer bei vielen Parametern, aber `?` Platzhalter sind schneller und der Standard.

> [!TIP]
> **Best Practice**: Verwende **IMMER** Prepared Statements f√ºr Benutzereingaben. Niemals String-Formatierung (`f"..."`, `.format()`, `%`) f√ºr SQL verwenden!

### UPDATE und DELETE Operationen

Neben `INSERT` und `SELECT` (aus V19) sind `UPDATE` (√Ñndern) und `DELETE` (L√∂schen) essenzielle Operationen.

#### UPDATE: Daten √§ndern

**Syntax**:
```sql
UPDATE Tabelle SET Spalte1 = Wert1, Spalte2 = Wert2 WHERE Bedingung
```

**Beispiel**: Maschine auf "inaktiv" setzen:

```python
maschinen_id = 5
cursor.execute('UPDATE Maschinen SET Aktiv = 0 WHERE Maschinen_ID = ?', (maschinen_id,))
conn.commit()
print(f"{cursor.rowcount} Zeile(n) aktualisiert")
```

`cursor.rowcount` gibt die Anzahl betroffener Zeilen zur√ºck.

**Beispiel**: Wartungsintervall f√ºr alle Fr√§sen verl√§ngern:

```python
cursor.execute('''
    UPDATE Maschinen 
    SET Wartungsintervall_Tage = Wartungsintervall_Tage + 30 
    WHERE Typ = ?
''', ('Fr√§se',))
conn.commit()
print(f"{cursor.rowcount} Maschine(n) aktualisiert")
```

> [!WARNING]
> **Vorsicht ohne WHERE-Klausel**: `UPDATE Maschinen SET Aktiv = 0` (ohne WHERE) √§ndert **alle** Zeilen! Immer WHERE-Bedingung verwenden, au√üer du willst wirklich alle Zeilen √§ndern.

**Bedingte Updates basierend auf aktuellen Werten**:

```python
# Erh√∂he Wartungskosten um 10% f√ºr Maschinen √§lter als 10 Jahre
cursor.execute('''
    UPDATE Wartungsprotokolle 
    SET Kosten_Euro = Kosten_Euro * 1.10 
    WHERE Maschinen_ID IN (
        SELECT Maschinen_ID FROM Maschinen 
        WHERE Baujahr < ?
    )
''', (2014,))  # 2024 - 10 = 2014
conn.commit()
```

#### DELETE: Daten l√∂schen

**Syntax**:
```sql
DELETE FROM Tabelle WHERE Bedingung
```

**Beispiel**: Wartungsprotokoll l√∂schen:

```python
protokoll_id = 10
cursor.execute('DELETE FROM Wartungsprotokolle WHERE Protokoll_ID = ?', (protokoll_id,))
conn.commit()
print(f"{cursor.rowcount} Zeile(n) gel√∂scht")
```

**Beispiel**: Alle inaktiven Maschinen l√∂schen:

```python
cursor.execute('DELETE FROM Maschinen WHERE Aktiv = 0')
conn.commit()
print(f"{cursor.rowcount} Maschine(n) gel√∂scht")
```

> [!WARNING]
> **CASCADE DELETE**: Wenn Fremdschl√ºssel-Constraints existieren, kann das L√∂schen eines Datensatzes fehlschlagen, wenn abh√§ngige Datens√§tze in anderen Tabellen existieren. L√∂sung: Entweder erst abh√§ngige Datens√§tze l√∂schen oder `ON DELETE CASCADE` im Schema definieren.

**Foreign Key Constraints in SQLite** (m√ºssen explizit aktiviert werden):

```python
cursor.execute('PRAGMA foreign_keys = ON')
```

Ohne dies werden Fremdschl√ºssel-Constraints in SQLite ignoriert!

**Beispiel mit CASCADE**:

```sql
CREATE TABLE Wartungsprotokolle (
    Protokoll_ID INTEGER PRIMARY KEY,
    Maschinen_ID INTEGER,
    Wartungsdatum TEXT,
    FOREIGN KEY (Maschinen_ID) REFERENCES Maschinen(Maschinen_ID) 
        ON DELETE CASCADE
);
```

Jetzt werden beim L√∂schen einer Maschine automatisch alle zugeh√∂rigen Wartungsprotokolle gel√∂scht.

### Transaktionen: Commit und Rollback

Wie in der Theorie besprochen, sind **Transaktionen** fundamental f√ºr Datenintegrit√§t. In Python mit `sqlite3` funktionieren Transaktionen folgenderma√üen:

**Explizite Transaktionen**:

```python
conn = sqlite3.connect('produktionsdb.db')
cursor = conn.cursor()

try:
    # Transaktion beginnt implizit beim ersten modifizierenden Statement
    cursor.execute('UPDATE Lagerbestand SET Menge = Menge - 10 WHERE Artikel_ID = ?', (5,))
    cursor.execute('INSERT INTO Buchungen (Artikel_ID, Menge, Datum) VALUES (?, ?, ?)', 
                   (5, -10, '2024-12-15'))
    
    # Beide Operationen erfolgreich ‚Üí Speichern
    conn.commit()
    print("Transaktion erfolgreich committed")

except sqlite3.Error as e:
    # Bei Fehler: R√ºckg√§ngig machen
    conn.rollback()
    print(f"Fehler: {e}. Transaktion wurde zur√ºckgerollt.")

finally:
    conn.close()
```

**Wichtig**: In SQLite beginnt eine Transaktion automatisch beim ersten `INSERT`, `UPDATE` oder `DELETE`. Sie wird mit `.commit()` gespeichert oder mit `.rollback()` r√ºckg√§ngig gemacht.

> [!NOTE]
> **Isolation Level** in SQLite: SQLite unterst√ºtzt nur `DEFERRED`, `IMMEDIATE` und `EXCLUSIVE` Transaktionen. Der Standard ist `DEFERRED` (Locks werden erst bei erstem Write gesetzt). F√ºr meiste Anwendungen ist dies ausreichend.

**Beispiel: Bank√ºberweisungs-Simulation**:

```python
def ueberweise(von_konto, nach_konto, betrag):
    try:
        cursor.execute('UPDATE Konten SET Saldo = Saldo - ? WHERE Konto_ID = ?', 
                       (betrag, von_konto))
        
        if cursor.rowcount == 0:
            raise ValueError("Quellkonto existiert nicht")
        
        cursor.execute('SELECT Saldo FROM Konten WHERE Konto_ID = ?', (von_konto,))
        neuer_saldo = cursor.fetchone()[0]
        
        if neuer_saldo < 0:
            raise ValueError("Unzureichende Deckung")
        
        cursor.execute('UPDATE Konten SET Saldo = Saldo + ? WHERE Konto_ID = ?', 
                       (betrag, nach_konto))
        
        if cursor.rowcount == 0:
            raise ValueError("Zielkonto existiert nicht")
        
        conn.commit()
        print(f"{betrag} Euro von Konto {von_konto} nach {nach_konto} √ºberwiesen")
    
    except (sqlite3.Error, ValueError) as e:
        conn.rollback()
        print(f"√úberweisung fehlgeschlagen: {e}")

# Verwendung
ueberweise(1, 2, 100.0)
```

Falls einer der Schritte fehlschl√§gt (z.B. Zielkonto existiert nicht), wird das Geld nicht abgebucht ‚Äì Atomarit√§t ist garantiert.

### Cursor-Objekte und fetchall() / fetchone()

Aus V19 wissen wir bereits, dass `.execute()` ein Cursor-Objekt zur√ºckgibt und `.fetchall()` alle Ergebnisse als Liste liefert. Hier vertiefen wir die Cursor-Funktionalit√§t.

**Cursor-Methoden im Detail**:

**`.fetchone()`**: Gibt die n√§chste Zeile als Tupel zur√ºck oder `None` bei Ende:

```python
cursor.execute('SELECT * FROM Maschinen WHERE Aktiv = 1')

zeile = cursor.fetchone()
while zeile:
    print(zeile)
    zeile = cursor.fetchone()
```

**`.fetchall()`**: Gibt alle verbleibenden Zeilen als Liste zur√ºck:

```python
cursor.execute('SELECT Name, Baujahr FROM Maschinen')
zeilen = cursor.fetchall()

for name, baujahr in zeilen:
    print(f"{name} ({baujahr})")
```

**`.fetchmany(size)`**: Gibt bis zu `size` Zeilen zur√ºck (n√ºtzlich f√ºr gro√üe Ergebnisse):

```python
cursor.execute('SELECT * FROM Messdaten')

while True:
    zeilen = cursor.fetchmany(1000)  # Blockweise 1000 Zeilen
    if not zeilen:
        break
    
    for zeile in zeilen:
        verarbeite(zeile)
```

Dies ist speichereffizienter als `.fetchall()` bei Millionen Zeilen.

**Cursor als Iterator**:

```python
cursor.execute('SELECT * FROM Maschinen')

for zeile in cursor:
    print(zeile)
```

Dies ist √§quivalent zu wiederholtem `.fetchone()`, aber pythonischer.

**`.rowcount`**: Anzahl betroffener Zeilen bei `UPDATE`/`DELETE` oder `-1` bei `SELECT` (in SQLite):

```python
cursor.execute('DELETE FROM Maschinen WHERE Aktiv = 0')
print(f"{cursor.rowcount} Zeilen gel√∂scht")
```

**`.description`**: Metadaten √ºber Spalten (Name, Typ, etc.):

```python
cursor.execute('SELECT Name, Baujahr FROM Maschinen')
print(cursor.description)
# Ausgabe: (('Name', None, None, None, None, None, None), ('Baujahr', None, None, None, None, None, None))

spalten = [desc[0] for desc in cursor.description]
print(spalten)  # ['Name', 'Baujahr']
```

**Zugriff nach Spaltennamen mit `row_factory`** (aus V19 bekannt, hier vertieft):

```python
conn.row_factory = sqlite3.Row
cursor = conn.cursor()

cursor.execute('SELECT Name, Baujahr FROM Maschinen WHERE Maschinen_ID = ?', (1,))
zeile = cursor.fetchone()

if zeile:
    print(zeile['Name'])    # Zugriff nach Name
    print(zeile[0])         # Zugriff nach Index (auch m√∂glich)
    print(dict(zeile))      # Als Dictionary ausgeben
```

`sqlite3.Row` verh√§lt sich wie ein Tupel, unterst√ºtzt aber auch Dictionary-√§hnlichen Zugriff. Dies macht Code lesbarer bei vielen Spalten.

### Context Manager f√ºr sichere Verbindungen

**Context Manager** (`with`-Statement) garantieren, dass Ressourcen korrekt freigegeben werden, auch wenn Fehler auftreten. Dies ist Best Practice f√ºr Datenbankverbindungen.

**Ohne Context Manager** (fehleranf√§llig):

```python
conn = sqlite3.connect('daten.db')
cursor = conn.cursor()
cursor.execute('INSERT INTO Tabelle VALUES (?)', (wert,))
conn.commit()
conn.close()  # Wird bei Exception NICHT ausgef√ºhrt!
```

**Mit Context Manager** (sicher):

```python
with sqlite3.connect('daten.db') as conn:
    cursor = conn.cursor()
    cursor.execute('INSERT INTO Tabelle VALUES (?)', (wert,))
    conn.commit()
# conn.close() wird automatisch aufgerufen
```

> [!WARNING]
> **Wichtig**: Der Context Manager f√ºr `sqlite3.connect()` ruft bei Verlassen des Blocks **nicht** `.close()` auf, sondern nur `.commit()` (bei Erfolg) oder `.rollback()` (bei Exception). F√ºr vollst√§ndiges Cleanup manuell `.close()` in `finally` oder zweiten Context Manager verwenden.

**Empfohlenes Pattern**:

```python
try:
    with sqlite3.connect('daten.db') as conn:
        cursor = conn.cursor()
        
        # Datenbank-Operationen
        cursor.execute('INSERT INTO Maschinen (Name, Typ) VALUES (?, ?)', ('CNC-07', 'Fr√§se'))
        # Kein manuelles conn.commit() n√∂tig ‚Äì passiert automatisch
        
except sqlite3.IntegrityError as e:
    print(f"Constraint-Verletzung: {e}")
except sqlite3.Error as e:
    print(f"Datenbankfehler: {e}")
finally:
    if 'conn' in locals():
        conn.close()
```

**Noch besser: Eigene Context Manager-Klasse**:

```python
class DatenbankVerbindung:
    def __init__(self, db_pfad):
        self.db_pfad = db_pfad
        self.conn = None
    
    def __enter__(self):
        self.conn = sqlite3.connect(self.db_pfad)
        self.conn.row_factory = sqlite3.Row
        return self.conn
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type is None:
            self.conn.commit()
        else:
            self.conn.rollback()
        self.conn.close()

# Verwendung
with DatenbankVerbindung('daten.db') as conn:
    cursor = conn.cursor()
    cursor.execute('SELECT * FROM Maschinen')
    for zeile in cursor:
        print(dict(zeile))
```

Dies garantiert immer Commit bei Erfolg, Rollback bei Exception und Close am Ende.

### Aggregationen und GROUP BY in Python

Aggregat-Funktionen (`COUNT`, `SUM`, `AVG`, `MIN`, `MAX`) und `GROUP BY` wurden in V19 theoretisch eingef√ºhrt. Hier zeigen wir praktische Anwendung in Python.

**Beispiel: Statistiken √ºber Wartungen**:

```python
# Anzahl Wartungen pro Maschine
cursor.execute('''
    SELECT m.Name, COUNT(w.Protokoll_ID) AS Anzahl_Wartungen
    FROM Maschinen m
    LEFT JOIN Wartungsprotokolle w ON m.Maschinen_ID = w.Maschinen_ID
    GROUP BY m.Maschinen_ID, m.Name
    ORDER BY Anzahl_Wartungen DESC
''')

for zeile in cursor.fetchall():
    print(f"{zeile[0]}: {zeile[1]} Wartungen")
```

**Beispiel: Durchschnittliche Kosten pro Maschinentyp**:

```python
cursor.execute('''
    SELECT m.Typ, 
           COUNT(w.Protokoll_ID) AS Anzahl,
           AVG(w.Kosten_Euro) AS Durchschnitt_Kosten,
           SUM(w.Kosten_Euro) AS Gesamt_Kosten
    FROM Maschinen m
    INNER JOIN Wartungsprotokolle w ON m.Maschinen_ID = w.Maschinen_ID
    GROUP BY m.Typ
    HAVING COUNT(w.Protokoll_ID) > 5
    ORDER BY Gesamt_Kosten DESC
''')

conn.row_factory = sqlite3.Row
for zeile in cursor.fetchall():
    print(f"{zeile['Typ']}: "
          f"{zeile['Anzahl']} Wartungen, "
          f"√ò {zeile['Durchschnitt_Kosten']:.2f} ‚Ç¨, "
          f"Gesamt {zeile['Gesamt_Kosten']:.2f} ‚Ç¨")
```

**HAVING-Klausel**: Filtert Gruppen nach Aggregation. `WHERE` filtert vor Gruppierung, `HAVING` danach.

```python
# Maschinen mit mehr als 3 Wartungen im letzten Jahr
cursor.execute('''
    SELECT m.Name, COUNT(w.Protokoll_ID) AS Anzahl
    FROM Maschinen m
    INNER JOIN Wartungsprotokolle w ON m.Maschinen_ID = w.Maschinen_ID
    WHERE w.Wartungsdatum >= date('now', '-1 year')
    GROUP BY m.Maschinen_ID, m.Name
    HAVING COUNT(w.Protokoll_ID) > 3
''')
```

> [!TIP]
> **Performance-Tipp**: Bei gro√üen Tabellen k√∂nnen Aggregationen langsam sein. Erstelle Indizes auf Spalten in GROUP BY und JOIN-Bedingungen.

### JOINs: Daten aus mehreren Tabellen verkn√ºpfen

**INNER JOIN**: Nur Zeilen mit √úbereinstimmung in beiden Tabellen:

```python
cursor.execute('''
    SELECT m.Name AS Maschine, 
           w.Wartungsdatum, 
           w.Wartungstyp, 
           w.Kosten_Euro
    FROM Maschinen m
    INNER JOIN Wartungsprotokolle w ON m.Maschinen_ID = w.Maschinen_ID
    WHERE m.Typ = ?
    ORDER BY w.Wartungsdatum DESC
''', ('Fr√§se',))

for zeile in cursor.fetchall():
    print(f"{zeile[0]} | {zeile[1]} | {zeile[2]} | {zeile[3]} ‚Ç¨")
```

**LEFT JOIN**: Alle Zeilen aus linker Tabelle + √úbereinstimmungen aus rechter:

```python
# Alle Maschinen + ihre Wartungen (auch wenn keine Wartungen vorhanden)
cursor.execute('''
    SELECT m.Name, 
           COUNT(w.Protokoll_ID) AS Anzahl_Wartungen
    FROM Maschinen m
    LEFT JOIN Wartungsprotokolle w ON m.Maschinen_ID = w.Maschinen_ID
    GROUP BY m.Maschinen_ID, m.Name
''')

for zeile in cursor.fetchall():
    anzahl = zeile[1] if zeile[1] else 0
    print(f"{zeile[0]}: {anzahl} Wartungen")
```

Maschinen ohne Wartungen haben `COUNT = 0` (wegen LEFT JOIN). Bei INNER JOIN w√ºrden sie nicht auftauchen.

**Mehrere JOINs**:

```python
# Maschinen + Wartungen + zust√§ndige Techniker
cursor.execute('''
    SELECT m.Name AS Maschine,
           w.Wartungsdatum,
           t.Name AS Techniker,
           w.Kosten_Euro
    FROM Maschinen m
    INNER JOIN Wartungsprotokolle w ON m.Maschinen_ID = w.Maschinen_ID
    INNER JOIN Techniker t ON w.Techniker_ID = t.Techniker_ID
    ORDER BY w.Wartungsdatum DESC
    LIMIT 10
''')
```

> [!WARNING]
> **JOIN-Performance**: Ohne Indizes auf Fremdschl√ºsseln k√∂nnen JOINs extrem langsam werden. Stelle sicher, dass Fremdschl√ºssel-Spalten indexiert sind:
> 
> ```python
> cursor.execute('CREATE INDEX idx_wartung_maschinen_id ON Wartungsprotokolle(Maschinen_ID)')
> ```

### Subqueries (Unterabfragen)

**Subquery in WHERE**:

```python
# Maschinen, deren Wartungskosten √ºber dem Durchschnitt liegen
cursor.execute('''
    SELECT m.Name, SUM(w.Kosten_Euro) AS Gesamt_Kosten
    FROM Maschinen m
    INNER JOIN Wartungsprotokolle w ON m.Maschinen_ID = w.Maschinen_ID
    GROUP BY m.Maschinen_ID, m.Name
    HAVING SUM(w.Kosten_Euro) > (
        SELECT AVG(Gesamt) FROM (
            SELECT SUM(Kosten_Euro) AS Gesamt
            FROM Wartungsprotokolle
            GROUP BY Maschinen_ID
        )
    )
''')
```

**Subquery mit IN**:

```python
# Wartungen f√ºr Maschinen eines bestimmten Typs
cursor.execute('''
    SELECT Wartungsdatum, Wartungstyp, Kosten_Euro
    FROM Wartungsprotokolle
    WHERE Maschinen_ID IN (
        SELECT Maschinen_ID FROM Maschinen WHERE Typ = ?
    )
''', ('Fr√§se',))
```

Subqueries k√∂nnen JOINs ersetzen, sind aber meist langsamer. Verwende JOINs, wenn m√∂glich.

### Best Practices und h√§ufige Fehler

**1. Immer Prepared Statements verwenden**

```python
# FALSCH (SQL-Injection!):
name = input("Name: ")
cursor.execute(f"SELECT * FROM Maschinen WHERE Name = '{name}'")

# RICHTIG:
cursor.execute('SELECT * FROM Maschinen WHERE Name = ?', (name,))
```

**2. Transaktionen f√ºr zusammenh√§ngende Operationen**

```python
# Mehrere Operationen in einer Transaktion
try:
    cursor.execute('INSERT INTO Maschinen (...) VALUES (...)', (...))
    cursor.execute('INSERT INTO Wartungsprotokolle (...) VALUES (...)', (...))
    conn.commit()  # Beide zusammen speichern
except sqlite3.Error:
    conn.rollback()  # Beide zur√ºckrollen
```

**3. Ressourcen freigeben**

```python
# Immer schlie√üen
conn.close()

# Oder Context Manager verwenden
with sqlite3.connect('daten.db') as conn:
    # ... Operationen ...
    pass  # Automatisches Cleanup
```

**4. Indizes f√ºr Performance**

```python
# Erstelle Indizes auf h√§ufig gefilterte Spalten
cursor.execute('CREATE INDEX idx_typ ON Maschinen(Typ)')
cursor.execute('CREATE INDEX idx_wartung_datum ON Wartungsprotokolle(Wartungsdatum)')
```

**5. Error Handling**

```python
try:
    cursor.execute('INSERT INTO Maschinen (Maschinen_ID, Name) VALUES (?, ?)', (1, 'Test'))
    conn.commit()
except sqlite3.IntegrityError as e:
    print(f"Constraint-Verletzung: {e}")  # z.B. Duplikat-ID
except sqlite3.OperationalError as e:
    print(f"SQL-Fehler: {e}")  # z.B. Tabelle existiert nicht
except sqlite3.Error as e:
    print(f"Allgemeiner Datenbankfehler: {e}")
```

**6. Foreign Key Constraints aktivieren (SQLite-spezifisch)**

```python
conn = sqlite3.connect('daten.db')
cursor = conn.cursor()
cursor.execute('PRAGMA foreign_keys = ON')  # Standardm√§√üig OFF in SQLite!
```

Ohne dies werden Fremdschl√ºssel-Verletzungen nicht erkannt.

**7. Batch-Inserts mit executemany()**

```python
# LANGSAM (einzelne Transaktionen):
for daten in datenliste:
    cursor.execute('INSERT INTO Tabelle VALUES (?)', (daten,))
    conn.commit()

# SCHNELL (eine Transaktion):
cursor.executemany('INSERT INTO Tabelle VALUES (?)', [(d,) for d in datenliste])
conn.commit()
```

`executemany()` ist 10-100x schneller bei vielen Inserts.

**8. row_factory f√ºr Lesbarkeit**

```python
conn.row_factory = sqlite3.Row
cursor = conn.cursor()
cursor.execute('SELECT Name, Baujahr FROM Maschinen')
for zeile in cursor:
    print(zeile['Name'], zeile['Baujahr'])  # Lesbar!
```

**9. LIMIT f√ºr gro√üe Ergebnisse**

```python
# Vermeide Speicher-Overflow bei Millionen Zeilen
cursor.execute('SELECT * FROM Messdaten LIMIT 1000')
```

Oder verwende `.fetchmany(size)` f√ºr Streaming.

**10. Backup vor kritischen Operationen**

```python
import shutil
shutil.copy('produktionsdb.db', 'produktionsdb_backup.db')

# Kritische Operation
cursor.execute('DELETE FROM ...')
conn.commit()
```

> [!TIP]
> **Debugging-Tipp**: Gib SQL-Queries mit Parametern aus, bevor du sie ausf√ºhrst:
> 
> ```python
> query = 'SELECT * FROM Maschinen WHERE Typ = ?'
> params = ('Fr√§se',)
> print(f"Query: {query} | Params: {params}")
> cursor.execute(query, params)
> ```

### Praktische √úbung: Datenbank f√ºr KI-Training-Logs

Wir erstellen eine Datenbank zur Verwaltung von Trainings-Logs f√ºr Machine-Learning-Modelle. Dies ist ein realistisches Anwendungsszenario in der Industrie.

**Schema**:

```python
import sqlite3
from datetime import datetime

conn = sqlite3.connect('ml_training_logs.db')
cursor = conn.cursor()

# Tabelle: Modelle
cursor.execute('''
    CREATE TABLE IF NOT EXISTS Modelle (
        Modell_ID INTEGER PRIMARY KEY AUTOINCREMENT,
        Modellname TEXT NOT NULL UNIQUE,
        Architektur TEXT NOT NULL,
        Erstellt_Am TEXT DEFAULT CURRENT_TIMESTAMP
    )
''')

# Tabelle: Training-Runs
cursor.execute('''
    CREATE TABLE IF NOT EXISTS Training_Runs (
        Run_ID INTEGER PRIMARY KEY AUTOINCREMENT,
        Modell_ID INTEGER NOT NULL,
        Hyperparameter TEXT,  -- JSON-String
        Start_Zeit TEXT NOT NULL,
        End_Zeit TEXT,
        Dauer_Sekunden REAL,
        Status TEXT CHECK(Status IN ('laufend', 'abgeschlossen', 'abgebrochen', 'fehler')),
        FOREIGN KEY (Modell_ID) REFERENCES Modelle(Modell_ID)
    )
''')

# Tabelle: Metriken pro Epoche
cursor.execute('''
    CREATE TABLE IF NOT EXISTS Epochen_Metriken (
        Metrik_ID INTEGER PRIMARY KEY AUTOINCREMENT,
        Run_ID INTEGER NOT NULL,
        Epoche INTEGER NOT NULL,
        Train_Loss REAL,
        Val_Loss REAL,
        Train_Accuracy REAL,
        Val_Accuracy REAL,
        Learning_Rate REAL,
        FOREIGN KEY (Run_ID) REFERENCES Training_Runs(Run_ID) ON DELETE CASCADE,
        UNIQUE(Run_ID, Epoche)
    )
''')

# Indizes f√ºr Performance
cursor.execute('CREATE INDEX IF NOT EXISTS idx_runs_modell ON Training_Runs(Modell_ID)')
cursor.execute('CREATE INDEX IF NOT EXISTS idx_metriken_run ON Epochen_Metriken(Run_ID)')

# Foreign Keys aktivieren
cursor.execute('PRAGMA foreign_keys = ON')

conn.commit()
print("Datenbank erstellt")
```

**Daten einf√ºgen**:

```python
import json
import time

# Modell registrieren
cursor.execute('''
    INSERT INTO Modelle (Modellname, Architektur) 
    VALUES (?, ?)
''', ('Predictive_Maintenance_v1', 'LSTM'))
modell_id = cursor.lastrowid

# Training-Run starten
hyperparams = json.dumps({'batch_size': 32, 'learning_rate': 0.001, 'epochs': 50})
cursor.execute('''
    INSERT INTO Training_Runs (Modell_ID, Hyperparameter, Start_Zeit, Status)
    VALUES (?, ?, ?, ?)
''', (modell_id, hyperparams, datetime.now().isoformat(), 'laufend'))
run_id = cursor.lastrowid
conn.commit()

# Epochen-Metriken simulieren
for epoche in range(1, 51):
    train_loss = 0.5 * (0.95 ** epoche) + 0.01  # Simulierte Konvergenz
    val_loss = train_loss + 0.05
    
    cursor.execute('''
        INSERT INTO Epochen_Metriken (Run_ID, Epoche, Train_Loss, Val_Loss, Train_Accuracy, Val_Accuracy, Learning_Rate)
        VALUES (?, ?, ?, ?, ?, ?, ?)
    ''', (run_id, epoche, train_loss, val_loss, 0.8 + epoche*0.004, 0.75 + epoche*0.003, 0.001))
    
    if epoche % 10 == 0:
        conn.commit()  # Batch-Commit alle 10 Epochen

# Training abschlie√üen
end_zeit = datetime.now().isoformat()
dauer = 3600.5  # Simuliert 1 Stunde
cursor.execute('''
    UPDATE Training_Runs 
    SET End_Zeit = ?, Dauer_Sekunden = ?, Status = ?
    WHERE Run_ID = ?
''', (end_zeit, dauer, 'abgeschlossen', run_id))
conn.commit()

print(f"Training-Run {run_id} abgeschlossen")
```

**Abfragen**:

```python
# Bestes Modell basierend auf finaler Val-Accuracy
cursor.execute('''
    SELECT m.Modellname, 
           tr.Run_ID,
           MAX(em.Val_Accuracy) AS Beste_Accuracy
    FROM Modelle m
    INNER JOIN Training_Runs tr ON m.Modell_ID = tr.Modell_ID
    INNER JOIN Epochen_Metriken em ON tr.Run_ID = em.Run_ID
    WHERE tr.Status = 'abgeschlossen'
    GROUP BY m.Modell_ID, tr.Run_ID
    ORDER BY Beste_Accuracy DESC
    LIMIT 5
''')

print("Top 5 Runs:")
for zeile in cursor.fetchall():
    print(f"{zeile[0]} (Run {zeile[1]}): {zeile[2]:.4f} Val-Accuracy")

# Konvergenz-Analyse: Letzte 10 Epochen eines Runs
cursor.execute('''
    SELECT Epoche, Train_Loss, Val_Loss, Val_Accuracy
    FROM Epochen_Metriken
    WHERE Run_ID = ?
    ORDER BY Epoche DESC
    LIMIT 10
''', (run_id,))

print(f"\nLetzte 10 Epochen von Run {run_id}:")
for zeile in cursor.fetchall():
    print(f"Epoche {zeile[0]}: Train Loss {zeile[1]:.4f}, Val Loss {zeile[2]:.4f}, Val Acc {zeile[3]:.4f}")
```

Diese Datenbank erm√∂glicht Experiment-Tracking f√ºr ML-Projekte ‚Äì ein kritischer Bestandteil moderner KI-Entwicklung.

---

## üîö Zusammenfassung

In dieser Vorlesung haben wir fortgeschrittene Datenbankkonzepte und SQL-Techniken behandelt:

**Theoretische Konzepte**:
- **Normalisierung** (1NF, 2NF, 3NF) zur Vermeidung von Redundanz und Anomalien
- **Indizes** zur Beschleunigung von Abfragen (B-Trees, Unique, Composite, Partial)
- **Transaktionen und ACID** (Atomicity, Consistency, Isolation, Durability)
- **NoSQL-Datenbanken** (Key-Value, Document, Column-Family, Graph) und CAP-Theorem
- **Vector Databases** f√ºr Embeddings und Similarity Search (RAG, semantische Suche)

**Python-Praxis**:
- **Prepared Statements** gegen SQL-Injection (IMMER `?` Platzhalter verwenden)
- **UPDATE und DELETE** Operationen mit WHERE-Klauseln
- **Transaktionen** explizit steuern (`.commit()`, `.rollback()`)
- **Cursor-Methoden** (`.fetchone()`, `.fetchall()`, `.fetchmany()`)
- **Context Manager** f√ºr sichere Ressourcenverwaltung
- **Aggregationen und JOINs** f√ºr komplexe Abfragen
- **Best Practices**: Indizes, Error Handling, `executemany()`, `row_factory`

Mit diesem Wissen k√∂nnen Sie professionelle, sichere und performante Datenbankanwendungen entwickeln ‚Äì ob relationale Datenbanken f√ºr strukturierte Daten oder NoSQL-L√∂sungen f√ºr spezielle Anwendungsf√§lle wie KI-Embeddings.

