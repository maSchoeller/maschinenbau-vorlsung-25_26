# V20 ‚Äì Aufgaben: Datenbanken ‚Äì Teil 2

---

## üìö Theorie-Aufgaben

### T1: Normalisierung einer Produktions-Datenbank ‚≠ê‚≠ê

**Kontext**: Ein Maschinenbau-Unternehmen hat eine unnormalisierte Tabelle `Produktionsauftraege`, die Informationen √ºber Fertigungsauftr√§ge, Artikel und Maschinen mischt.

**Tabelle `Produktionsauftraege` (unnormalisiert)**:

| Auftrag_ID | Auftragsdatum | Kunde_Name | Kunde_Adresse | Artikel_Name | Artikel_Preis | Artikel_Gewicht_kg | Maschine_Name | Maschine_Typ | Menge | Liefertermin |
|------------|---------------|------------|---------------|--------------|---------------|-------------------|---------------|--------------|-------|--------------|
| 1001       | 2024-01-10    | M√ºller AG  | Hauptstr. 10  | Zahnrad Z42  | 125.50        | 2.3               | CNC-Fr√§se 01  | Fr√§se        | 50    | 2024-02-10   |
| 1001       | 2024-01-10    | M√ºller AG  | Hauptstr. 10  | Welle W15    | 89.00         | 5.1               | Drehmaschine 02 | Drehbank   | 30    | 2024-02-10   |
| 1002       | 2024-01-15    | Schmidt GmbH | Bahnhofstr. 5| Zahnrad Z42  | 125.50        | 2.3               | CNC-Fr√§se 03  | Fr√§se        | 100   | 2024-03-01   |
| 1003       | 2024-01-20    | M√ºller AG  | Hauptstr. 10  | Geh√§use G08  | 245.00        | 12.5              | Drehmaschine 02 | Drehbank   | 20    | 2024-02-28   |

**Aufgaben**:

a) **Identifikation von Anomalien**: Beschreibe mindestens **drei Probleme** (Update-, Insert- oder Delete-Anomalien), die in dieser Tabelle auftreten k√∂nnen. Gib f√ºr jede Anomalie ein konkretes Beispiel.

b) **Erste Normalform (1NF)**: Zeige, ob die Tabelle in 1NF ist. Falls nicht, beschreibe, was verletzt wird, und erstelle ein Schema in 1NF.

c) **Zweite Normalform (2NF)**: Analysiere, ob partielle Abh√§ngigkeiten existieren (nehme an, `Auftrag_ID` alleine ist **nicht** ausreichend als Prim√§rschl√ºssel, sondern `(Auftrag_ID, Artikel_Name)` bildet den zusammengesetzten Schl√ºssel f√ºr Auftragspositionen). Welche Spalten h√§ngen nur von einem Teil des Schl√ºssels ab? Erstelle ein Schema in 2NF mit mindestens **zwei separaten Tabellen**.

d) **Dritte Normalform (3NF)**: Identifiziere transitive Abh√§ngigkeiten in deinem 2NF-Schema. Erstelle ein vollst√§ndiges 3NF-Schema mit **mindestens vier Tabellen** (`Kunden`, `Artikel`, `Maschinen`, `Auftraege`, `Auftragspositionen` o.√§.). Gib f√ºr jede Tabelle Prim√§rschl√ºssel und Fremdschl√ºssel an.

e) **Begr√ºndung**: Erkl√§re in 2-3 S√§tzen, warum das 3NF-Schema praktische Vorteile gegen√ºber der unnormalisierten Tabelle hat (z.B. Konsistenz, Wartbarkeit, Speicherplatz).

---

### T2: Indizes und Query-Optimierung ‚≠ê‚≠ê‚≠ê

**Kontext**: Eine Produktionsdatenbank mit 500.000 Maschinen-Messwerten pro Tag (Tabelle `Sensordaten`) leidet unter langsamen Abfragen. Die Datenbank-Administratorin m√∂chte Indizes strategisch einsetzen.

**Tabelle `Sensordaten`**:
```sql
CREATE TABLE Sensordaten (
    Messung_ID INTEGER PRIMARY KEY AUTOINCREMENT,
    Maschinen_ID INTEGER NOT NULL,
    Sensor_Typ TEXT NOT NULL,
    Messwert REAL NOT NULL,
    Zeitstempel TEXT NOT NULL,
    Status TEXT CHECK(Status IN ('normal', 'warnung', 'kritisch'))
);
```

**Typische Abfragen** (sortiert nach H√§ufigkeit):

1. `SELECT * FROM Sensordaten WHERE Maschinen_ID = 42 AND Zeitstempel >= '2024-12-01' ORDER BY Zeitstempel DESC LIMIT 100;` (50% aller Queries)
2. `SELECT AVG(Messwert) FROM Sensordaten WHERE Sensor_Typ = 'Temperatur' AND Status = 'kritisch';` (30% aller Queries)
3. `SELECT Maschinen_ID, COUNT(*) FROM Sensordaten WHERE Zeitstempel >= '2024-12-15' GROUP BY Maschinen_ID;` (15% aller Queries)
4. `INSERT INTO Sensordaten (Maschinen_ID, Sensor_Typ, Messwert, Zeitstempel, Status) VALUES (...);` (5% aller Queries, aber sehr h√§ufig ‚Äì Batch-Inserts)

**Aufgaben**:

a) **Index-Analyse**: F√ºr jede der vier Queries, begr√ºnde, ob ein Index helfen w√ºrde und **welcher Index** am besten geeignet w√§re. Ber√ºcksichtige:
   - Einspalten- vs. Mehrspaltiger Index
   - Reihenfolge der Spalten bei Composite Index
   - Trade-offs (schnellere SELECTs vs. langsamere INSERTs)

b) **Index-Empfehlung**: Erstelle **maximal drei Indizes**, die das Gesamtsystem optimieren. Schreibe die `CREATE INDEX`-Statements. Begr√ºnde, warum du genau diese Indizes gew√§hlt hast und warum du nicht mehr erstellst.

c) **Partial Index (PostgreSQL-Feature)**: Angenommen, 95% aller Messungen haben Status "normal", nur 5% haben "warnung" oder "kritisch". Schlage einen Partial Index vor, der Query 2 beschleunigt, aber weniger Speicher verbraucht als ein vollst√§ndiger Index auf `(Sensor_Typ, Status)`.

d) **Denormalisierung**: Die Datenbank-Administratorin schl√§gt vor, eine zus√§tzliche Tabelle `Sensordaten_Aggregate` zu erstellen, die t√§glich vorbere vorberechnete Durchschnittswerte pro Maschine und Sensor speichert. Diskutiere **zwei Vorteile** und **zwei Nachteile** dieser Denormalisierung (je 1-2 S√§tze).

e) **Monitoring**: Wie w√ºrdest du √ºberpr√ºfen, ob deine Indizes tats√§chlich genutzt werden? Nenne das SQL-Keyword/Command f√ºr PostgreSQL oder SQLite.

---

### T3: Transaktionen und ACID in kritischen Systemen ‚≠ê‚≠ê‚≠ê

**Kontext**: Ein automatisiertes Lagerverwaltungssystem f√ºr Maschinenbauteile muss sicherstellen, dass Lagerbest√§nde immer korrekt sind, selbst bei gleichzeitigen Zugriffen von mehreren Robotern.

**Datenbank-Schema**:
```sql
CREATE TABLE Lagerbestand (
    Artikel_ID INTEGER PRIMARY KEY,
    Artikelname TEXT NOT NULL,
    Menge_Lager INTEGER NOT NULL CHECK(Menge_Lager >= 0),
    Mindestbestand INTEGER NOT NULL
);

CREATE TABLE Buchungen (
    Buchung_ID INTEGER PRIMARY KEY AUTOINCREMENT,
    Artikel_ID INTEGER NOT NULL,
    Menge_Aenderung INTEGER NOT NULL,  -- Positiv bei Einlagerung, negativ bei Entnahme
    Zeitstempel TEXT NOT NULL,
    Roboter_ID TEXT,
    FOREIGN KEY (Artikel_ID) REFERENCES Lagerbestand(Artikel_ID)
);
```

**Szenario**: Zwei Roboter (R1 und R2) greifen gleichzeitig auf dieselbe Schraube (Artikel-ID 101, aktueller Bestand: 50) zu. Beide wollen 40 Schrauben entnehmen.

**Transaktions-Ablauf ohne Isolation**:
1. R1 liest Bestand: 50 (ausreichend f√ºr 40)
2. R2 liest Bestand: 50 (ausreichend f√ºr 40)
3. R1 entnimmt 40 ‚Üí aktualisiert Bestand auf 10
4. R2 entnimmt 40 ‚Üí aktualisiert Bestand auf 10 (√ºberschreibt R1's Update!)
5. **Problem**: 80 Schrauben wurden entnommen, aber nur 40 sind im System verbucht!

**Aufgaben**:

a) **ACID-Verletzung**: Welche der vier ACID-Eigenschaften wird in diesem Szenario verletzt? Erkl√§re in 2-3 S√§tzen, warum dies kritisch ist.

b) **Isolation Level**: Beschreibe, welcher **Isolation Level** (READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, SERIALIZABLE) dieses Problem l√∂sen w√ºrde. Was garantiert dieser Level konkret?

c) **Pessimistische L√∂sung**: Entwerfe eine Transaktion mit **Pessimistic Locking** (verwende `SELECT ... FOR UPDATE` in SQL-Pseudocode), die sicherstellt, dass nur der erste Roboter erfolgreich ist, und der zweite eine Fehlermeldung erh√§lt ("Bestand nicht ausreichend").

d) **Optimistische L√∂sung**: F√ºge eine Spalte `Version` zur `Lagerbestand`-Tabelle hinzu. Entwerfe eine Transaktion, die Optimistic Locking verwendet: Der UPDATE erfolgt nur, wenn die `Version` noch √ºbereinstimmt. Bei Konflikt muss die Anwendung neu lesen und erneut versuchen. Zeige SQL-Pseudocode f√ºr dieses Pattern.

e) **Deadlock-Szenario**: Beschreibe ein Szenario mit **zwei** Robotern und **zwei** Artikeln, bei dem ein Deadlock entstehen kann (R1 sperrt Artikel A und wartet auf B, R2 sperrt B und wartet auf A). Schlage eine Strategie vor, um Deadlocks zu vermeiden (Tipp: Zugriffsreihenfolge).

f) **Durability**: Erkl√§re, wie ein DBMS Durability garantiert, wenn w√§hrend einer Transaktion (nach Bestandsupdate, aber vor COMMIT) der Strom ausf√§llt. Nenne das verwendete Konzept.

---

## üêç Python-Aufgaben

**Hinweis**: Alle Python-Aufgaben arbeiten mit der Datenbank `produktionsdb.db` aus dem Ordner `testdaten/`. Diese Datenbank enth√§lt Tabellen f√ºr Maschinen, Sensordaten, Wartungen und Produktionsl√§ufe. Die Struktur findest du in `testdaten/TESTDATEN_README.md`.

---

### P1: SQL-Injection Schwachstelle beheben ‚≠ê‚≠ê

**Szenario**: Ein Legacy-System f√ºr die Qualit√§tskontrolle hat eine Sicherheitsl√ºcke. Der folgende Code erlaubt SQL-Injection-Angriffe:

```python
import sqlite3

def suche_artikel(artikelname):
    """
    UNSICHERER CODE - NUR ZU DEMONSTRATIONSZWECKEN!
    Sucht Artikel nach Name und gibt Pr√ºfprotokolle zur√ºck.
    """
    conn = sqlite3.connect('testdaten/produktionsdb.db')
    cursor = conn.cursor()
    
    # GEF√ÑHRLICH: String-Formatierung in SQL!
    query = f"SELECT * FROM Pruefprotokolle WHERE Artikelname = '{artikelname}'"
    cursor.execute(query)
    
    ergebnisse = cursor.fetchall()
    conn.close()
    return ergebnisse

# Beispiel-Aufruf (normal):
print(suche_artikel("Zahnrad Z42"))

# ANGRIFF: Ein b√∂swilliger Benutzer gibt ein:
# artikelname = "' OR '1'='1"
# Query wird zu: SELECT * FROM Pruefprotokolle WHERE Artikelname = '' OR '1'='1'
# -> Gibt ALLE Protokolle zur√ºck, nicht nur das gesuchte!
```

**Aufgaben**:

a) **Exploit demonstrieren**: Erkl√§re in 2-3 S√§tzen, wie ein Angreifer mit dem Eingabestring `"' OR '1'='1"` **alle** Pr√ºfprotokolle abrufen k√∂nnte, obwohl er nur Zugriff auf einen Artikel haben sollte.

b) **Kritischer Exploit**: Beschreibe einen noch gef√§hrlicheren Angriff, bei dem der Eingabestring `"'; DROP TABLE Pruefprotokolle; --"` verwendet wird. Was passiert mit der Datenbank?

c) **Sichere Implementierung**: Schreibe die Funktion `suche_artikel_sicher(artikelname)` neu, die **Prepared Statements** mit `?` Platzhaltern verwendet. Stelle sicher, dass auch bei b√∂sartigen Eingaben keine SQL-Injection m√∂glich ist.

d) **Test**: Teste deine sichere Funktion mit den Eingaben:
   - Normaler Name: `"Zahnrad Z42"`
   - Angriff 1: `"' OR '1'='1"`
   - Angriff 2: `"'; DROP TABLE Pruefprotokolle; --"`
   
   Dokumentiere die Ausgabe (wie viele Zeilen werden jeweils zur√ºckgegeben, und tritt ein Fehler auf?).

e) **Bonus**: Implementiere eine zus√§tzliche Funktion `suche_artikel_erweitert(artikelname, min_pruefwert)`, die nach Artikelname **und** einem Mindest-Pr√ºfwert filtert. Verwende **Named Placeholders** (`:artikelname`, `:min_wert`).

---

### P2: UPDATE und DELETE mit Fehlerbehandlung ‚≠ê‚≠ê

**Szenario**: Eine Wartungs-Management-Anwendung muss Maschinendaten aktualisieren und alte Wartungsprotokolle l√∂schen.

**Aufgaben**:

a) **Betriebsstunden aktualisieren**: Schreibe eine Funktion `aktualisiere_betriebsstunden(maschinen_id, neue_stunden)`, die:
   - Die Spalte `Betriebsstunden` in der Tabelle `Maschinen` f√ºr die gegebene `maschinen_id` aktualisiert
   - Pr√ºft, ob die Maschine existiert (nutze `cursor.rowcount` nach dem UPDATE)
   - Bei Erfolg: `"Maschine {id}: {alte_stunden}h ‚Üí {neue_stunden}h"` ausgibt
   - Bei Misserfolg: `"Maschine {id} nicht gefunden"` ausgibt
   - Die Transaktion committed

b) **Wartungen l√∂schen**: Schreibe eine Funktion `loesche_alte_wartungen(tage_alt)`, die:
   - Alle Eintr√§ge aus der Tabelle `Wartungen` l√∂scht, deren `Datum` √§lter als `tage_alt` Tage ist
   - Die Anzahl gel√∂schter Zeilen zur√ºckgibt (`cursor.rowcount`)
   - `sqlite3.IntegrityError` abf√§ngt (falls Foreign Key Constraints verletzt werden)
   - Bei Fehler: Transaktion rollbackt und eine Fehlermeldung ausgibt

c) **Maschine au√üer Betrieb setzen**: Schreibe eine Funktion `deaktiviere_maschine_mit_wartungen(maschinen_id)`, die:
   - Die Spalte `Aktiv` in `Maschinen` auf `0` setzt
   - **Zus√§tzlich** einen Eintrag in `Wartungen` erstellt mit:
     - `Maschinen_ID = maschinen_id`
     - `Wartungstyp = 'Stilllegung'`
     - `Datum = datetime.now().strftime('%Y-%m-%d')`
     - `Beschreibung = 'Maschine au√üer Betrieb gesetzt'`
   - Beide Operationen in **einer Transaktion** durchf√ºhrt (beide m√ºssen erfolgreich sein, sonst Rollback)
   - Bei Erfolg: `"Maschine {id} wurde deaktiviert und Wartungseintrag erstellt"` ausgibt
   - Bei Fehler: Rollback + Fehlermeldung

d) **Fehlerbehandlung testen**: Teste `loesche_alte_wartungen(365)` und provoziere bewusst einen `IntegrityError`:
   - Aktiviere Foreign Keys: `PRAGMA foreign_keys = ON;`
   - Versuche, Wartungen zu l√∂schen, die √ºber Foreign Keys mit anderen Tabellen verkn√ºpft sind
   - Dokumentiere, welcher Fehler auftritt und wie deine Funktion damit umgeht

e) **Cascade DELETE**: √Ñndere das Schema der Tabelle `Wartungen` so, dass beim L√∂schen einer Maschine **automatisch** alle zugeh√∂rigen Wartungen gel√∂scht werden. Gib das `ALTER TABLE`- oder `CREATE TABLE`-Statement mit `ON DELETE CASCADE` an.

---

### P3: Transaktionen mit Rollback ‚≠ê‚≠ê‚≠ê

**Szenario**: Ein automatisiertes Produktionsplanungssystem muss einen Produktionslauf starten. Dabei werden mehrere Tabellen gleichzeitig ge√§ndert: Materialverbrauch wird gebucht, Maschinenstatus wird aktualisiert, und ein neuer Produktionslauf wird angelegt. **Alle drei Operationen m√ºssen erfolgreich sein** ‚Äì bei einem Fehler darf **keine** der √Ñnderungen persistiert werden.

**Tabellen** (siehe `testdaten/produktionsdb.db`):
- `Materialbestand` (Material_ID, Materialname, Menge_Lager, Einheit)
- `Maschinen` (Maschinen_ID, Name, Status TEXT)  -- Status: 'Bereit', 'Produktion', 'Wartung'
- `Produktionslaeufe` (Lauf_ID, Maschinen_ID, Artikel, Menge_Geplant, Start_Zeit, Status TEXT)

**Aufgaben**:

a) **Transaktion implementieren**: Schreibe eine Funktion `starte_produktionslauf(maschinen_id, artikel, menge, material_verbrauch)`, die:
   - Parameter `material_verbrauch` ist ein Dictionary: `{'Material_ID': menge_verbraucht, ...}`
   - **Operation 1**: Pr√ºft, ob die Maschine im Status "Bereit" ist (sonst Exception `ValueError("Maschine nicht bereit")`)
   - **Operation 2**: Reduziert den `Materialbestand` f√ºr alle Materialien in `material_verbrauch`. Wirft `ValueError("Materialbestand nicht ausreichend")`, falls Menge negativ werden w√ºrde.
   - **Operation 3**: Aktualisiert Maschinenstatus auf "Produktion"
   - **Operation 4**: F√ºgt einen neuen Eintrag in `Produktionslaeufe` ein (Start_Zeit = aktuelles Datum, Status = 'Laufend')
   - Bei **jedem Fehler**: Komplettes Rollback aller vier Operationen
   - Bei Erfolg: Commit und Ausgabe `"Produktionslauf {lauf_id} gestartet auf Maschine {maschinen_id}"`

b) **Rollback testen**: Teste deine Funktion mit den folgenden Szenarien:
   - **Erfolgsfall**: Maschine 1 ist bereit, Material ausreichend vorhanden ‚Üí Alle Operationen erfolgreich
   - **Fehlerfall 1**: Maschine 1 ist bereits in "Produktion" ‚Üí Rollback, keine √Ñnderungen in DB
   - **Fehlerfall 2**: Material-ID 101 hat nur 50 Einheiten, aber du versuchst 100 zu verbrauchen ‚Üí Rollback
   
   Pr√ºfe nach jedem Test mit SELECT-Queries, ob die Datenbank unver√§ndert bleibt bei Fehlern.

c) **Nested Transactions (Savepoints)**: Erweitere die Funktion um **Savepoints**: Falls Material-Operation fehlschl√§gt, rollback nur bis zu einem Savepoint nach der Maschinen-Pr√ºfung (nicht komplett). Nutze `SAVEPOINT` und `ROLLBACK TO` in SQL.

d) **Logging**: F√ºge eine Tabelle `Transaktions_Log` hinzu (Log_ID, Zeitstempel, Operation, Status TEXT, Fehler TEXT). Protokolliere **jeden** Versuch (erfolgreich oder fehlgeschlagen) mit Commit/Rollback-Status.

e) **Context Manager**: Erstelle eine eigene Context Manager Klasse `ProduktionsTransaktion`, die automatisch Rollback bei Exceptions durchf√ºhrt:
   ```python
   with ProduktionsTransaktion('testdaten/produktionsdb.db') as db:
       db.cursor.execute(...)
       # Bei Exception automatisch Rollback
   ```

---

### P4: Aggregationen und Visualisierung mit pandas + matplotlib ‚≠ê‚≠ê‚≠ê

**Szenario**: Die Produktionsleitung m√∂chte die **Wartungskosten** nach Maschinentyp und Quartal analysieren, um Budgets f√ºr 2025 zu planen.

**Datenbank**: `testdaten/produktionsdb.db`

**Tabellen**:
- `Maschinen` (Maschinen_ID, Name, Typ TEXT, Baujahr, Aktiv)
- `Wartungen` (Wartung_ID, Maschinen_ID FK, Wartungstyp TEXT, Datum TEXT, Kosten REAL, Beschreibung)

**Aufgaben**:

a) **Daten abfragen**: Schreibe eine SQL-Query mit JOIN, die folgende Informationen extrahiert:
   - Maschinentyp (`Typ` aus `Maschinen`)
   - Quartal aus Wartungsdatum (extrahiere Jahr und Quartal aus `Datum`)
   - Summe der Wartungskosten (`SUM(Kosten)`)
   - Anzahl der Wartungen (`COUNT(*)`)
   
   Gruppiere nach Typ und Quartal. Die Query sollte etwa so aussehen:
   ```sql
   SELECT 
       m.Typ,
       strftime('%Y', w.Datum) || '-Q' || ((CAST(strftime('%m', w.Datum) AS INTEGER) - 1) / 3 + 1) AS Quartal,
       SUM(w.Kosten) AS Gesamt_Kosten,
       COUNT(*) AS Anzahl_Wartungen
   FROM Maschinen m
   INNER JOIN Wartungen w ON m.Maschinen_ID = w.Maschinen_ID
   WHERE ...
   GROUP BY ...
   ```

b) **pandas DataFrame**: Lade die Ergebnisse in einen pandas DataFrame:
   ```python
   import pandas as pd
   import sqlite3
   
   conn = sqlite3.connect('testdaten/produktionsdb.db')
   df = pd.read_sql_query(query, conn)
   ```

c) **Datenbereinigung**: 
   - Konvertiere die Spalte `Gesamt_Kosten` zu `float`
   - Filtere nur Quartale aus 2023 und 2024
   - Sortiere nach Typ und Quartal

d) **Pivot-Tabelle**: Erstelle eine Pivot-Tabelle mit:
   - Index: Quartal (z.B. "2023-Q1", "2023-Q2", ...)
   - Spalten: Maschinentyp (z.B. "Fr√§se", "Drehbank", "Roboter")
   - Werte: Summe der Wartungskosten
   
   Verwende `pd.pivot_table()` mit `aggfunc='sum'` und `fill_value=0`.

e) **Visualisierung**: Erstelle **zwei Plots** mit matplotlib:
   
   **Plot 1**: Gestapeltes Balkendiagramm (`plt.bar()` mit `bottom`-Parameter)
   - X-Achse: Quartale (2023-Q1 bis 2024-Q4)
   - Y-Achse: Wartungskosten in Euro
   - Gestapelt nach Maschinentyp (unterschiedliche Farben)
   - Titel: "Wartungskosten nach Quartal und Maschinentyp"
   - Legende mit Maschinentypen
   - Speichere als `wartungskosten_quartal.png`
   
   **Plot 2**: Liniendiagramm f√ºr Trend-Analyse
   - Gruppiere nach Typ, zeige Gesamtkosten √ºber Zeit
   - Mehrere Linien (eine pro Maschinentyp)
   - Markiere Maximalwerte mit `plt.annotate()`
   - Speichere als `wartungskosten_trend.png`

f) **Statistik-Ausgabe**: Berechne und ausgebe:
   - Durchschnittliche Kosten pro Wartung nach Typ (Gesamt_Kosten / Anzahl_Wartungen)
   - Typ mit h√∂chsten Gesamtkosten
   - Quartal mit h√∂chsten Gesamtkosten √ºber alle Typen
   
   Nutze pandas-Methoden wie `.mean()`, `.max()`, `.idxmax()`.

g) **Export**: Exportiere die Pivot-Tabelle als Excel-Datei `wartungskosten_analyse.xlsx` (nutze `df.to_excel()`).

---

### P5: Projekt ‚Äì Qualit√§tskontroll-Dashboard mit JOINs und Subqueries ‚≠ê‚≠ê‚≠ê‚≠ê

**Szenario**: Du sollst ein **Qualit√§tskontroll-Dashboard** f√ºr eine Fertigungslinie entwickeln, das Pr√ºfprotokolle, Maschinen und Produktionsl√§ufe verkn√ºpft. Das System muss folgende Anforderungen erf√ºllen:

1. **Datenbank-Schema erweitern**: Erg√§nze die bestehende `produktionsdb.db` um Qualit√§tsdaten
2. **Komplexe Queries mit JOINs**: Verkn√ºpfe mehrere Tabellen f√ºr Reports
3. **Subqueries f√ºr Analysen**: Finde "Problem-Maschinen" und "Top-Artikel"
4. **Visualisierung**: Erstelle ein Dashboard mit pandas + matplotlib

**Bestehende Tabellen** (siehe `testdaten/produktionsdb.db`):
- `Maschinen` (Maschinen_ID, Name, Typ, Baujahr, Aktiv)
- `Produktionslaeufe` (Lauf_ID, Maschinen_ID FK, Artikel, Menge_Geplant, Start_Zeit, Status)

**Neue Tabellen** (erstelle diese):

```sql
CREATE TABLE Pruefprotokolle (
    Pruef_ID INTEGER PRIMARY KEY AUTOINCREMENT,
    Lauf_ID INTEGER NOT NULL,
    Artikel TEXT NOT NULL,
    Pruef_Datum TEXT NOT NULL,
    Pruef_Typ TEXT NOT NULL,  -- 'Sichtpr√ºfung', 'Ma√üpr√ºfung', 'Funktionstest'
    Pruef_Wert REAL,           -- Z.B. gemessene L√§nge in mm, H√§rte HRC, etc.
    Soll_Wert REAL,            -- Sollwert laut Spezifikation
    Toleranz REAL,             -- Zul√§ssige Abweichung (+/-)
    Status TEXT CHECK(Status IN ('Bestanden', 'Fehlgeschlagen', 'Nacharbeit')),
    Pruefer_Name TEXT,
    Bemerkung TEXT,
    FOREIGN KEY (Lauf_ID) REFERENCES Produktionslaeufe(Lauf_ID) ON DELETE CASCADE
);

CREATE TABLE Fehlerarten (
    Fehler_ID INTEGER PRIMARY KEY AUTOINCREMENT,
    Pruef_ID INTEGER NOT NULL,
    Fehlerart TEXT NOT NULL,  -- 'Ma√üabweichung', 'Oberfl√§chenfehler', 'Funktionsst√∂rung'
    Schweregrad INTEGER CHECK(Schweregrad BETWEEN 1 AND 5),  -- 1=gering, 5=kritisch
    Beschreibung TEXT,
    FOREIGN KEY (Pruef_ID) REFERENCES Pruefprotokolle(Pruef_ID) ON DELETE CASCADE
);
```

**Aufgaben**:

#### a) Schema und Beispieldaten (20 Punkte)

1. Erstelle die beiden neuen Tabellen `Pruefprotokolle` und `Fehlerarten` in `produktionsdb.db`
2. Erstelle Indizes auf:
   - `Pruefprotokolle(Lauf_ID)`
   - `Pruefprotokolle(Status)`
   - `Fehlerarten(Pruef_ID)`
   - `Fehlerarten(Fehlerart)`
3. F√ºge mindestens **30 Pr√ºfprotokolle** f√ºr verschiedene Produktionsl√§ufe ein:
   - Mindestens 3 verschiedene Artikel ("Zahnrad Z42", "Welle W15", "Geh√§use G08")
   - Mindestens 3 verschiedene Pr√ºftypen
   - Status-Verteilung: ca. 70% "Bestanden", 20% "Fehlgeschlagen", 10% "Nacharbeit"
   - Realistische Werte mit Toleranzen (z.B. Soll: 50.00mm, Toleranz: ¬±0.05mm, Ist: 50.03mm ‚Üí "Bestanden")
4. F√ºge f√ºr jeden fehlgeschlagenen Test mindestens einen Eintrag in `Fehlerarten` ein

#### b) Komplexe JOIN-Queries (30 Punkte)

Schreibe folgende Queries:

**Query 1**: **Qualit√§ts√ºbersicht nach Maschine**
- Zeige: Maschinen-Name, Maschinen-Typ, Anzahl Produktionsl√§ufe, Anzahl Pr√ºfungen, Anzahl bestandene/fehlgeschlagene Tests, Fehlerquote (%)
- Verkn√ºpfe: `Maschinen` ‚Üí `Produktionslaeufe` ‚Üí `Pruefprotokolle`
- Gruppiere nach Maschine
- Sortiere nach Fehlerquote absteigend (schlechteste Maschinen zuerst)

**Query 2**: **Problem-Artikel identifizieren**
- Zeige: Artikel-Name, Gesamtanzahl Pr√ºfungen, Anzahl Fehler, H√§ufigste Fehlerart, Durchschnittlicher Schweregrad
- Nutze Subquery: Finde die h√§ufigste Fehlerart mit `GROUP BY` und `COUNT(*)`, dann `MAX()`
- Filtere nur Artikel mit mehr als 3 fehlgeschlagenen Tests

**Query 3**: **Pr√ºfer-Performance**
- Zeige: Pr√ºfer-Name, Anzahl Pr√ºfungen, Anzahl Fehlerfunde, Durchschnittliche Pr√ºf-Zeit (falls Zeitstempel vorhanden)
- Sortiere nach Anzahl Fehlerfunde absteigend

#### c) Subqueries f√ºr Analyse (20 Punkte)

**Query 4**: **Maschinen mit √ºberdurchschnittlicher Fehlerquote**
- Nutze Subquery, um zuerst die durchschnittliche Fehlerquote **√ºber alle Maschinen** zu berechnen
- Hauptquery: Zeige nur Maschinen, deren Fehlerquote dar√ºber liegt
- Beispiel-Struktur:
  ```sql
  SELECT m.Name, (Fehler / Total * 100.0) AS Fehlerquote
  FROM ...
  WHERE (Fehler / Total * 100.0) > (
      SELECT AVG(Fehler / Total * 100.0) FROM ...
  )
  ```

**Query 5**: **Artikel, die auf allen Maschinen getestet wurden**
- Nutze Subquery oder `HAVING COUNT(DISTINCT Maschinen_ID) = (SELECT COUNT(*) FROM Maschinen WHERE Aktiv = 1)`
- Zeige nur Artikel, die auf mindestens 3 verschiedenen Maschinen produziert wurden

#### d) pandas DataFrame und Aggregationen (15 Punkte)

1. Lade Query 1 (Qualit√§ts√ºbersicht) in pandas DataFrame
2. Berechne zus√§tzliche Kennzahlen:
   - First Pass Yield (FPY): Anteil bestandene Tests beim ersten Versuch
   - Mean Time Between Failures (MTBF): Durchschnittliche Anzahl bestandene Tests zwischen Fehlern
3. Filtere Maschinen mit FPY < 80% (kritische Maschinen)
4. Exportiere als CSV: `qualitaetsuebersicht.csv`

#### e) Visualisierung (15 Punkte)

Erstelle **drei Plots**:

**Plot 1**: Balkendiagramm ‚Äì Fehlerquote nach Maschine
- X-Achse: Maschinen-Name
- Y-Achse: Fehlerquote in %
- Farbe: Gr√ºn (<5%), Gelb (5-10%), Rot (>10%)
- Horizontale Linie bei 5% als Soll-Wert

**Plot 2**: Heatmap ‚Äì Fehlerarten nach Artikel
- Zeilen: Artikel-Namen
- Spalten: Fehlerarten
- Farbe: Anzahl Fehler (verwende `plt.imshow()` mit Colorbar)

**Plot 3**: Zeitreihe ‚Äì Fehlerquote √ºber Zeit
- X-Achse: Wochen oder Monate
- Y-Achse: Fehlerquote %
- Mehrere Linien f√ºr verschiedene Artikel
- Markiere Wochen mit >15% Fehlerquote

Speichere alle Plots als PNG-Dateien.

---

**Abgabe**:
- `p5_qualitaetskontrolle.py` (Hauptskript mit allen Funktionen)
- `qualitaetsuebersicht.csv` (Export aus pandas)
- `fehlerquote_maschinen.png`, `fehlerarten_heatmap.png`, `fehlerquote_zeit.png` (Plots)
- `testdaten/produktionsdb.db` (erweiterte Datenbank mit neuen Tabellen und Daten)

**Bonus** (5 Punkte): Erstelle eine interaktive Streamlit-App, die die Queries ausf√ºhrt und Plots anzeigt.

---

